\section{Rappresentazione di polinomi}
% polinomi definizione 33.7
In generale, avere diverse rappresentazioni di un oggetto permette di eseguire operazioni diverse su rappresentazioni diverse, dove risulta più comodo. Chiaramente la rappresentazione è legata a come si possono fare le operazioni, e sono inoltre necessarie operazioni efficienti di conversione.

Introduciamo quindi un nuovo dominio applicativo, i polinomi, su cui verrà costruito l'algoritmo per la trasformata veloce di \textit{Fourier}.

\begin{definition}[Polinomio]
    Un polinomio è una funzione $p: \mathbb{C} \rightarrow \mathbb{C}$ definita su un'indeterminata e un insieme di coefficienti, come somma di monomi.
    \begin{equation*}
        p(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_{n-1} x^{n-1} = \sum_{j=0}^{n-1}a_j x^j
    \end{equation*}
    \label{def:polinomio}
\end{definition}

\begin{definition}[Grado di un polinomio]
    Il grado di un polinomio è definito come l'indice massimo del coefficiente non nullo
    \begin{equation*}
        deg(p(x)) = \max \left\{ i: a_i \neq 0 \right\}
    \end{equation*}
    \label{def:poligrado}
\end{definition}

% grado limitato da n, pag 33.9
\begin{definition}[Polinomio di grado limitato da $n$]
    Un polinomio si dice di grado limitato da $n$ se il suo massimo grado può essere $n-1$
    \label{def:polilimitato}
\end{definition}

\subsection{Rappresentazione per coefficienti}
% rapp coeff 33.8
Un polinomio può essere rappresentato a $n$ coefficienti
\begin{equation*}
    p(x) \equiv \vec{a} \in \mathbb{C}^n
\end{equation*}
La rappresentazione può essere facilmente estesa con un'operazione di padding
\begin{equation*}
    p(x) \equiv \left( \vec{a}, 0_m \right) \in \mathbb{C}^{n+m}
\end{equation*}

\subsection{Rappresentazione per punti}
% forse non serve una subsubsection
\begin{theorem}[Teorema di interpolazione]
    Date $n$ coppie di punti $\left( x_i, y_i \right) \in \mathbb{C}^2 \text{ con }
    % x_i~\neq~x_j
    x_i \neq x_j
    % \forall~i~\neq~j
    \; \forall i \neq j
    \; \exists ! p(x)
    $
    di grado limitato da $n$ per cui
    $p(x_i) = y_i$
    detto polinomio interpolante.
    \label{teo:interpolazione}
\end{theorem}
C'è quindi una corrispondenza tra $n-uple$ di punti e un \emph{singolo} polinomio, quindi una $n-upla$ di punti è una rappresentazione di un polinomio di grado limitato da $n$
\begin{equation*}
    p(x) \equiv \left( \vec{x}, \vec{y}\right) \quad
    \vec{x}, \vec{y} \in \mathbb{C}^{n}, x_i \neq x_j \; \forall i \neq j
\end{equation*}
dove $\vec{x}$ si dice base della rappresentazione

Anche la rappresentazione per punti si può estendere
\begin{equation*}
    \left( \vec{x}, \vec{y}\right) \rightarrow
    \left( \vec{x}^E, \vec{y}^E\right) \quad
    \vec{x}^E, \vec{y}^E \in \mathbb{C}^{m}, x_i \neq x_j \; \forall i \neq j, \text{ con } m>n
\end{equation*}
Per estendere la rappresentazione occorre valutare il polinomio in $m-n$ punti aggiuntivi

\subsection{Conversione tra rappresentazioni}
Se si dispone della rappresentazione per coefficienti, è sufficiente valutare il polinomio in un certo numero di punti per ricavare la rappresentazione per punti, mentre se si dispone di una tabulazione occorre interpolare il polinomio. Eseguire questa conversione in maniera efficiente sarà argomento della sezione \ref{sez:conversione}

\section{Operazioni tra polinomi}

\subsection{Operazioni utilizzando la rappresentazione per coefficienti}
% somma sottrazione coefficienti 34.8
\subsubsection{Somma e sottrazione}
Siano $A(x) \equiv \vec{a}$ e $B(x) \equiv \vec{b}$, con $\vec{a}, \vec{b} \in \mathbb{C}^n$ e $C(x) \equiv \vec{c}$ omogeneo, con $\vec{c} \in \mathbb{C}^n$
\begin{align*}
    C(x) &= A(x) + B(x)
    = \sum_{j=0}^{n-1} a_j x^j + \sum_{j=0}^{n-1} b_j x^j 
    = \sum_{j=0}^{n-1} \left( a_j + b_j \right) x^j 
    = \sum_{j=0}^{n-1} c_j x^j 
\end{align*}
Per cui $C(x)$ è rappresentato dalla somma vettoriale delle rappresentazioni: $\vec{c} = \vec{a} + \vec{b}$

% prodotto coefficienti 35
\subsubsection{Prodotto}
Siano $A(x) \equiv \vec{a}$ e $B(x) \equiv \vec{b}$, con $\vec{a}, \vec{b} \in \mathbb{C}^n$, in questo caso $C(x)$ è un polinomio di grado limitato da $2n-1$, infatti $(n-1)+(n-1)+1$ (il limite di grado è di $1$ superiore al grado massimo) quindi $C(x) \equiv \vec{c}$, con $\vec{c} \in \mathbb{C}^{2n-1}$
\begin{align*}
    C(x) &= A(x) \cdot B(x)
    = \left( \sum_{j=0}^{n-1} a_j x^j \right) \cdot \left( \sum_{j=0}^{n-1} b_j x^j \right) 
\end{align*}
Da cui, per $ 0 \leq j \leq 2n-2$
\begin{align*}
    c_j &= \sum_{ \substack{k,h \\ k+h=j \\ 0 \leq k,h \leq n-1} } a_k b_h
    \intertext{che semplifichiamo notando che $h=j-k$}
    c_j &= \sum_{k=0}^{n-1} a_k b_{j-k}
    \intertext{ma questa sommatoria è valida solo sfruttando una convenzione per cui se $j-k$ è $<0$ o $\geq n$, il coefficiente $b_{j-k}$ assume valore $0$}
\end{align*}
Si possono cercare vincoli più stretti su $k$
\begin{equation*}
    \left\{ 
        \begin{array}[h]{l}
            0 \leq k \leq n-1 \\
            0 \leq j-k \leq n-1
        \end{array}
    \right.
    \rightarrow
    \left\{ 
        \begin{array}[h]{l}
            0 \leq k \leq n-1 \\
            j-n+1 \leq k \leq j
        \end{array}
    \right.
\end{equation*}
ossia
\begin{equation*}
    \max\left\{ 0, j-n+1 \right\} \leq k \leq \min \left\{ n-1, j \right\}
\end{equation*}
\begin{definition}[Convoluzione lineare]
    La convoluzione lineare è un operatore vettoriale definito come
    \begin{equation*}
        \vec{c}=\vec{a} * \vec{b} \rightarrow 0\leq j \leq 2n : c_j = \sum_{\max\left\{ 0, j-n+1 \right\}}^{\min \left\{ n-1, j \right\}} a_k b_{j-k}
        % ho dubbi su quel \leq 2n, cioè per j=2n k va da n+1 a n-1 quindi la somma è vuota, ed è corretto, ma comunque si è allungato senza dire niente a nessuno e mi da fastidio
    \end{equation*}
    \label{def:convlin}
\end{definition}
TODO ESERCIZIO: supponi $\vec{b} \in \mathbb{C}^m$, ricava i vincoli, dovrebbe venirti $\max \left\{ 0, j-m+1 \right\}$

% magia delle più nere questo calcolo di complessità
La complessità per calcolare direttamente la convoluzione risulta, contando il numero di operazioni: $n^2$ prodotti $+n^2$ somme $-(2n-1)$ somme che risparmi, per cui $T(n)=2n^2-2n+1$ o alternativamente $T(n)=n^2+(n-1)^2$, in cui si mette in evidenza il numero di prodotti $\left[ n^2 \right]$ e di somme $\left[ (n-1)^2 \right]$.

% algoritmo convoluzione lineare 36
L'algoritmo naive per implementare la convoluzione lineare risulta
\begin{algorithm}[H]
\caption{Convoluzione lineare}\label{alg:convlinnaive}
\begin{algorithmic}[1]
    \Procedure{DEF\_LIN\_CONV}{$\vec{a}, \vec{b}$}
        \State $n \gets \vec{a}.length$
        \For{$j \gets 0 $ to $ 2n-2 $ }
            \State $c_j \gets 0$
            \For{$k \gets \max\left\{ 0, j-n+1 \right\} $ to $ \min \left\{ n-1, j \right\} $ }
                \State $c_j \gets c_j + a_k b_{j-k}$
            \EndFor
        \EndFor
        \State return $ \vec{c}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
In quest'implementazione una somma è stata sprecata inizializzando $c_j=0$, si potrebbe inizializzare a $c_j=a_{\max\left\{ 0, j-n+1 \right\}}b_{j-\max\left\{ 0, j-n+1 \right\}}$

La complessità dell'algoritmo risulta $T_{DLC} = \Theta \left( n^2 \right)$
\subsection{Operazioni utilizzando la rappresentazione per punti}

% somma per punti 36.3
\subsubsection{Somma e sottrazione}
Siano $A(x) \equiv \left( \vec{x}, \vec{y}_A \right)$ e $B(x) \equiv \left( \vec{x}, \vec{y}_B \right)$ rappresentazioni omogenee, sulla stessa base.
\begin{align*}
    C(x) &= A(x) + B(x)
    \rightarrow C(x_i) = A(x_i)+B(x_i)
    \rightarrow y_{C_{i}}= y_{A_{i}}+ y_{B_{i}}
\end{align*}
Per cui $C(x)$ è rappresentato da $\left( \vec{x}, \vec{y}_A+\vec{y}_B \right)$

% prodotto per punti 36.5
\subsubsection{Prodotto}
La relazione $C(x) = A(x) \cdot B(x) $ è valida per ogni $x$, quindi anche per tutti i punti $x_i$ della base: $C(x_i) = A(x_i) \cdot B(x_i) $.
Tuttavia \emph{non} è sufficiente rappresentare $C$ come
$\left( \vec{x}, \vec{y}_A \odot \vec{y}_B \right)$,
infatti sono necessari $2n-1$ punti per definire il polinomio.

Considerando le rappresentazioni estese $A(x) \equiv \left( \vec{x}^E, \vec{y}_A^E \right)$ e $B(x) \equiv \left( \vec{x}^E, \vec{y}_B^E \right)$, con $\vec{x}^E, \vec{y}_A^E,  \vec{y}_B^E \in \mathbb{C}^{2n-1}$ si ottiene la rappresentazione lecita 
$C(x) \equiv \left( \vec{x}^E, \vec{y}^E_A \odot \vec{y}^E_B \right)$.

Per eseguire il prodotto sono quindi necessari solamente $2n-1$ prodotti, posto di avere a disposizione la rappresentazione estesa, portando a una complessità di $T(n) = \Theta \left( n \right)$

\section{Conversione tra rappresentazioni}\label{sez:conversione}
% convertire in modo lento e poi veloce pag 36.9
% grafo commutativo secsi 36.9
Se si vuole interpolare o valutare un polinomio in maniera generale, non è possibile raggiungere una complessità minore di $\Theta \left( n^2 \right)$. È necessario utilizzare basi particolari per poter accelerare l'algoritmo.

\subsection{Valutazione}
\subsubsection{Valutazione naive}
% valutazione naive 37.2
Ricordando che
    \begin{equation*}
        p(x) = \sum_{j=0}^{n-1}a_j x^j
    \end{equation*}
si ottiene direttamente un algoritmo dalla definizione
\begin{algorithm}[H]
\caption{Valutazione naive}\label{alg:valnaive}
\begin{algorithmic}[1]
    \Procedure{DEF\_VAL}{$\vec{a}, \bar{x}$}
        \State $n \gets \vec{a}.length$
        \State $y \gets a_0$
        \State $pow \gets 1$
        \For{$j \gets 1 $ to $ n-1 $ }
            \State $pow \gets pow \cdot \bar{x}$
            \State $y \gets y + a_j \cdot pow$
        \EndFor
        \State return $y$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
Vengono compiute $n-1$ iterazioni con una somma e due prodotti per ciclo.

% valutazione con horner 37.5
\subsubsection{Valutazione con \textit{Horner}}
Si può riscrivere un polinomio seguendo la regola di \textit{Horner} come 
    \begin{equation*}
        % p(x) = a_0 + x \left( a_1 + x \left( a_2 + \cdots + x 
        p(x) = a_0 + x ( a_1 + x ( a_2 + \cdots + x 
        % \left( a_{n-2} + x a_{n-1} \right) \right. \cdots \right)
        ( a_{n-2} + x a_{n-1} 
        \underbrace{ ) \cdots ) }_{\mathclap{ n-1 \text{ parentesi} } }
    \end{equation*}
si ottiene così l'algoritmo
\begin{algorithm}[H]
    \caption{Valutazione con \textit{Horner}}\label{alg:valhorner}
\begin{algorithmic}[1]
    \Procedure{HOR\_VAL}{$\vec{a}, \bar{x}$}
        \State $n \gets \vec{a}.length$
        \State $y \gets a_{n-1}$
        \For{$j \gets 2 $ to $ n $ }
            \State $y \gets a_{n-j} + \bar{x}y$
        \EndFor
        \State return $y$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
Vengono compiute $n-1$ iterazioni con una somma e un prodotto per ciclo, portando la complessità a $T_H(n) = 2(n-1)$

% aggregato su n punti 37.9
Questo algoritmo deve essere applicato per ciascuno degli $n$ punti, per cui la complessità della valutazione risulta $\Theta \left( n^2 \right)$

\subsection{Interpolazione}
\subsubsection{Interpolazione con \textit{Gauss}}
% interpolazione gauss 38.2
La formula \ref{def:polinomio} vale $\forall x$ e in particolare $\forall x_i \in \vec{x}$ base della rappresentazione
\begin{equation*}
    p(x) = \sum_{j=0}^{n-1}a_j x^j
    \rightarrow y_i = p(x_i) = \sum_{j=0}^{n-1}a_j x_i^j
    \quad 0 \leq i \leq n-1
\end{equation*}
Si può considerare $\vec{a}$ come incognita di un sistema lineare di $n$ equazioni:
\begin{equation*}
    X \vec{a} = \vec{y}
\end{equation*}
dove
\begin{equation*}
    X = \left( 
        \begin{array}[h]{ccccccc}
            1 & x_0 & x_0^2 & \cdots & x_0^j & \cdots & x_0^{n-1} \\
            \vdots &&&&&& \vdots\\
            1 & x_i & x_i^2 & \cdots & x_i^j & \cdots & x_i^{n-1} \\
            \vdots &&&&&& \vdots\\
            1 & x_n & x_n^2 & \cdots & x_n^j & \cdots & x_n^{n-1} 
        \end{array}
    \right)
\end{equation*}
per cui $X$ è una matrice di \textit{Vandermonde}, quindi invertibile se e solo se $x_i \neq x_j \; \forall i\neq j$, ipotesi verificata per definizione di base.

Utilizzando il metodo dell'eliminazione di \textit{Gauss}, occorrono $n$ operazioni \textit{pivot} su $n^2$ elementi, portando ad una complessità di $\Theta \left( n^3 \right)$

\subsubsection{Interpolazione con \textit{Lagrange}}
% interpolazione lagrange 38.7
\textit{Lagrange} ha trovato una formula chiusa per il polinomio interpolante
\begin{equation*}
    p(x) = \sum_{k=0}^{n-1} \frac{\left(
            \displaystyle
            \prod_{\substack{j=0 \\ j \neq k}}^{n-1} \left( x - x_j \right)
    \right)}{\left( 
            \displaystyle
            \prod_{\substack{j=0 \\ j \neq k}}^{n-1} \left( x_k - x_j \right)
    \right)}
    = \sum_{k=0}^{n-1} \frac{y_k}{Q_k(x_k)} q_k(x)
\end{equation*}
avendo definito
\begin{equation*}
    Q_k(x) = \prod_{\substack{j=0 \\ j \neq k}}^{n-1} \left( x - x_j \right)
\end{equation*}
e si può dimostrare che la formula si valuta in $\Theta \left( n^2 \right)$ (sulle dispense online c'è un utile ESERCIZIO da leggere)

\subsection{Conversioni con la \textit{Discrete Fourier Transform}}
% conversioni più veloci 39
Senza perdita di generalità, si possono considerare basi particolari su cui valutare e interpolare un polinomio. In particolare si sceglie una famiglia di vettori di taglia $n$ legata alla radice principale dell'unità:
\begin{equation*}
    \vec{\Omega}_n = 
    \left(
        \omega_n^0=1 , \omega_n, \omega_n^2, \cdots, \omega_n^{n-1} 
    \right)
\end{equation*}
dove $\omega_n$ è la radice principale $n-esima$ di $1$ nel campo complesso.
Basandosi su questa base si può definire la trasformata veloce di \textit{Fourier}

\begin{definition}[\textit{Discrete Fourier Transform}]
    Dato un polinomio $A(x)$ e la sua rappresentazione per punti $A(x) \equiv \vec{a} \in \mathbb{C}^n$, si definisce la trasformata veloce di \textit{Fourier} come la valutazione di $A(x)$ sui punti di $\vec{\Omega}_n$
    \begin{equation*}
        \vec{y} = DFT_n \left( \vec{a} \right) \quad \text{ con } \quad y_i = \sum_{j=0}^{n-1} a_j \left( \omega_n^{i} \right)^{j}
    \end{equation*}
    per cui $\vec{y} \in \mathbb{C}^n$ e $A(x) \equiv \left( \vec{\Omega}_n , \vec{y} \right)$ è una rappresentazione per punti di $A(x)$
    \label{def:dft}
\end{definition}
% TODO piccole note a pag 40.1
Gli $n$ vincoli introdotti nella definizione sono $n$ valutazioni di $A(x)$ nei punti della base $\vec{\Omega}_n$ e possono essere raccolti in una matrice di \textit{Vandermonde} detta \emph{matrice di \emph{Fourier} di ordine $n$}
\begin{equation}
    V\left( \vec{\Omega}_n \right) = \left( 
        \begin{array}[h]{ccccccc}
            1 & \omega_n^0 = 1 & 1 & \cdots & 1 & \cdots & 1 \\
            \vdots &&&&&& \vdots\\
            1 & \omega_n & \omega_n^2 & \cdots & \omega_n^j & \cdots & \omega_n^{n-1} \\
            \vdots &&&&&& \vdots\\
            1 & \omega_n^{i} & \omega_{n}^{i2} & \cdots & \omega_{n}^{ij} & \cdots & \omega_n^{i\left( n-1 \right)} \\
            \vdots &&&&&& \vdots\\
            1 & \omega_n^{n-1} & \omega_{n}^{\left( n-1 \right) 2} & \cdots & \omega_{n}^{\left( n-1 \right) j} & \cdots & \omega_n^{\left( n-1 \right) \left( n-1 \right)} \\
        \end{array}
    \right) = F_n
    \label{eq:matricefourier}
\end{equation}
Si può riscrivere la $DFT$ come operatore lineare
\begin{equation*}
    DFT_n\left( \vec{a} \right) = F_n \vec{a} = \vec{y}
\end{equation*}
Ricordando che $\vec{\Omega}_n$ è una base composta da punti distinti e $F_n$ è una matrice di \textit{Vandermonde}, questa è anche invertibile e si può definire la trasformata inversa
\begin{equation*}
    DFT_n^{-1} \left( \vec{y} \right) = \vec{a} = F_n^{-1} \vec{y}
\end{equation*}
che risulta l'operazione di interpolazione.

Nota: la $DFT_n\left( \vec{a} \right) $ appena definita è un operatore algebrico lineare, una descrizione astratta del problema computazionale, mentre $FFT \left( \vec{a} \right)$ è l'algoritmo per calcolarla. Mostreremo che $T_{FFT} \left( n \right) = \Theta \left( n \log_n \right)$

\section{Proprietà della base $\vec{\Omega}_n$}
Prima di implementare l'algoritmo è ancora necessario individuare la proprietà di sottostruttura, che permetta di valutare un polinomio in funzione di valutazioni di polinomi di grado minore.

\subsection{Radici complesse}
Ogni numero complesso $z \in \mathbb{C}$ può essere rappresentato con la sua rappresentazione algebrica $z = a+ib$, ma è di maggiore utilità in questo caso la rappresentazione polare
\begin{equation*}
    z = \rho \left( \cos \theta + i \sin \theta \right) = \rho e ^{i \theta} \quad
    \text{ con } \quad \rho = \sqrt{a^2+b^2} \; \text{ e } \; 0 \leq \theta < 2\pi
\end{equation*}
Con questa rappresentazione il prodotto è molto semplice, ma in particolare lo è l'esponenziazione:
\begin{equation*}
    z^n = \rho^n e^{in\theta}
\end{equation*}
da cui si ricavano facilmente le $n$ radici $n-esime$ di un numero complesso $z \in \mathbb{C} \setminus \left\{ 0 \right\}$
\begin{equation*}
    z_k = \rho_k e^{i \theta_k}
    \quad \rightarrow \quad 
    z_k^n = z = \rho e^{i\theta} = \rho_k^n e ^{i \left( n \theta_k - 2 \pi k \right)}
\end{equation*}
da cui si ricavano due vincoli su $\rho_k $ e $ \theta_k$ con $0 \leq k \leq n-1$
\[
    \left\{
        \begin{array}[h]{l}
            \rho = \rho_k^n \\[4pt]
            \theta = n \theta_k - 2 \pi k
        \end{array}
    \right.
    \quad \rightarrow \quad 
    \left\{
        \begin{array}[h]{l}
            \rho_k = \sqrt[n]{\rho} \\[4pt]
            \theta_k = \frac{\theta + 2 \pi k}{n} = \frac{\theta}{n} + 2 \pi \frac{k}{n}
        \end{array}
    \right.
\]
Le radici sono quindi le rotazioni della radice principale intorno all'origine. Nel caso che ci interessa, per $z=1$ risultano $\rho=1$ e $\theta = 0$ da cui
\begin{equation*}
    z_k = \sqrt[n]{1}  \; e^{i \left( \frac{0}{n} + \frac{2 \pi k}{n} \right)} 
    = e ^{i \frac{2 \pi k }{n}} \quad k=0,\cdots,n-1
\end{equation*}
e risultano
\begin{equation*}
    k=1 \quad \rightarrow \quad \omega_n= e^{i \frac{2 \pi}{n}}
\end{equation*}

\subsection{Proprietà delle radici $n-esime$ dell'unità}

\subsubsection{Modulo dell'esponente}

Moltiplicare l'argomento per $k$ corrisponde a esponenziare, e vale
\begin{equation*}
    % \omega_n^j = \omega_n^{j \mod n}
    % \omega_n^j = \omega_n^{j \! \! \! \! \mod n}
    \omega_n^j = \omega_n^{j \bmod n}
\end{equation*}
infatti è come aggiungere o sottrarre multipli di $2\pi$. Questo vale anche in negativo: $\omega_8^{6} = \omega_8^{-2} $

\subsubsection{Lemma di Cancellazione}
\begin{lemma}[Lemma di Cancellazione]
    $\forall n, d > 0$ e $k \geq 0$ vale 
    \begin{equation*}
    % $
        \omega_{dn}^{dk} = \omega_n^k
    % $
    \end{equation*}
    \label{teo:cancellazione}
\end{lemma}
\begin{proof}
    $e^{i 2 \pi \frac{dk}{dn}} = e^{i 2 \pi \frac{k}{n}}$
\end{proof}
In più vale il seguente corollario
\begin{lemma}
    Se $n$ è pari, $ \omega_{n}^{n/2} =1 $
\end{lemma}
\begin{proof}
    $ \omega_{n}^{n/2} =
    \omega_{2 \cdot n/2}^{1 \cdot n/2} =
    \omega_{2} = 1 $
\end{proof}
Nota: la radice principale quadrata dell'unità è $-1$

\subsubsection{Lemma di Dimezzamento}
\begin{lemma}
    Sia $n>0$, $n$ pari. Allora gli $n$ quadrati
    \begin{equation*}
        \left( \omega_{n}^{i} \right)^2 \quad 0 \leq i \leq n-1
    \end{equation*}
    coincidono a coppie, e in particolare
    \begin{equation*}
        \left( \omega_{n}^{i} \right)^2 = 
        \left( \omega_{n}^{i + n/2} \right)^2 = 
        \omega_{n/2}^{i}
    \end{equation*}
    \label{teo:dimezzamento}
\end{lemma}
\begin{proof}
    \begin{equation*}
        \left( \omega_{n}^{i} \right)^2 = 
        \omega_{n}^{2i}  = 
        \omega_{2 \cdot n/2}^{2 \cdot i}  = 
        \omega_{n/2}^{i}
    \end{equation*}
    \begin{equation*}
        \left( \omega_{n}^{i + n/2} \right)^2 = 
        \omega_{n}^{2i+n}  = 
        \omega_{n}^{2i} \cdot
        \cancel{ \omega_{n}^{n} } = 
        \omega_{2 \cdot n/2}^{2 \cdot i}  = 
        \omega_{n/2}^{i}
    \end{equation*}
\end{proof}

\subsubsection{Lemma di Somma}
\begin{lemma}
    Sia $n \geq 1$, $k \geq 0$
    \begin{equation*}
        \sum_{j=0}^{n-1} \left( \omega_n^k \right)^j = 
        \begin{cases}
            % n & \text{ se } k \mod n = 0 \\
            % n & \text{ se } k \pmod n = 0 \\
            n & \text{se } k \bmod n = 0 \\
            % n & \text{ se } k \mathrm{mod} n = 0 \\
            0 & \text{altrimenti}
        \end{cases}
    \end{equation*}
    \label{teo:somma}
\end{lemma}
\begin{proof}
    Nel primo caso $k \bmod n = 0$
    \begin{equation*}
        \omega_n^k = \omega_n^{k \bmod n} = \omega_n^0 = 1
        \quad \rightarrow \quad
        \sum_{j=0}^{n-1} 1 = n
    \end{equation*}
    Nel secondo caso $k \bmod n \neq 0$ e $\omega_n^k \neq 1$
    \begin{equation*}
        \sum_{j=0}^{n-1} \left( \omega_n^k \right)^j = 
        \frac{\left( \omega_n^k \right)^n -1}{\omega_n^k -1} =
        \frac{\left( \omega_n^n \right)^k -1}{\omega_n^k -1} =
        \frac{1 -1}{\omega_n^k -1} = 0
    \end{equation*}
\end{proof}

\section{Proprietà di sottostruttura}
Ricordando che $n=2^k$, $A(x) = \vec{a} \in \mathbb{C}^n$ e 
\begin{equation*}
    \vec{y} = DFT_n \left( \vec{a} \right) \quad \text{ con }
    \quad y_i = 
    A\left( \omega_n^i \right) = 
    \sum_{j=0}^{n-1} a_j \left( \omega_n^{i} \right)^{j}
\end{equation*}
si può sviluppare la proprietà di sottostruttura.
Per il caso base $n=1$
\begin{equation*}
    \vec{a} = a_0 \in \mathbb{C}^1
    \quad \rightarrow \quad
    % A(x) = a_0 |_{x=\omega_1^0} = a_0
    A(x) = a_0 \bigr|_{x=\omega_1^0} = a_0
\end{equation*}
ossia
\begin{equation*}
    \vec{y} = DFT_1 \left( \vec{a} \right) = \vec{a}
\end{equation*}
mentre per $n>1$ si può suddividere il vettore
$\vec{a}=\left( a_0, a_1,\cdots, a _{n-1} \right)$ in due vettori di lunghezza $n/2$ composti dalle sue componenti pari e dispari
\begin{equation*}
    \vec{a}^{[0]}=\left( a_0, a_2,\cdots, a _{n-2} \right)
    \quad \text{ e } \quad
    \vec{a}^{[1]}=\left( a_1, a_3,\cdots, a _{n-1} \right)
\end{equation*}
che avendo $n/2$ coefficienti sono di grado limitato da $n/2$

Si può quindi definire la seguente identità polinomiale su cui verrà costruita la proprietà di sottostruttura, che permetterà di ottenere $A(x)$ in funzione solamente di $A^{[0]} \left( x^2 \right)$ e $A^{[1]} \left( x^2 \right)$ 
\begin{definition}[Identità polinomiale]
    $\forall x \in \mathbb{C}$ vale
    \begin{equation*}
        A(x) = A^{[0]} \left( x^2 \right) + x \cdot A^{[1]} \left( x^2 \right) 
    \end{equation*}
    \label{def:identitapolinomiale}
\end{definition}
\begin{proof}
    I due polinomi sono definiti come
    \begin{equation*}
        A^{[0]}(x) = \sum_{j=0}^{\frac{n}{2}-1} a_{2j}x^j
        \quad \text{ e } \quad
        A^{[1]}(x) = \sum_{j=0}^{\frac{n}{2}-1} a_{2j+1}x^j
    \end{equation*}
    quindi valutando $A^{[0]}(x)$ in $x^2$ e sviluppando $x \cdot A^{[1]} \left( x^2 \right) $ risultano
    \begin{equation*}
        A^{[0]} \left( x^2 \right) = \sum_{j=0}^{\frac{n}{2}-1} a_{2j}x^{2j}
        \quad \text{ e } \quad
        x \cdot A^{[1]} \left( x^2 \right) =
        x \sum_{j=0}^{\frac{n}{2}-1} a_{2j+1}x^{2j} = 
        \sum_{j=0}^{\frac{n}{2}-1} a_{2j+1}x^{2j+1}
    \end{equation*}
    dove il primo contiene tutti i monomi pari di $A(x)$ e il secondo tutti i monomi dispari. Sommando i due termini si ottiene $A(x)$ come cercato.
\end{proof}

Il caso base è stato già trattato, supponiamo ora di aver già calcolato 
\begin{equation*}
    \vec{y}^{\,[0]} = DFT_{n/2} \left( \vec{a}^{[0]} \right)
    \quad \text{ e } \quad
    \vec{y}^{\,[1]} = DFT_{n/2} \left( \vec{a}^{[1]} \right)
\end{equation*}
mostriamo come ricavare
\begin{equation*}
    \vec{y} = DFT_{n} \left( \vec{a} \right)
\end{equation*}
utilizzando l'identità polinomiale.
In altri termini, si vogliono ottenere i valori incogniti 
\begin{equation*}
    y_i = A \left( \omega_{n}^{i} \right)
    \quad \forall \: 0 \leq i \leq n-1
\end{equation*}
a partire dai valori noti
\begin{equation*}
    y_i^{[0]} = A^{[0]} \left( \omega_{n/2}^{i} \right)
    \quad \text{ e } \quad
    y_i^{[1]} = A^{[1]} \left( \omega_{n/2}^{i} \right)
    \quad \forall \: 0 \leq i \leq \frac{n}{2}-1
\end{equation*}
Sia $0 \leq i \leq \frac{n}{2}-1$
\begin{align*}
    y_i &= A \left( \omega_{n}^{i} \right) 
    & \text{per l'identità polinomiale}
    \\
    &= A^{[0]} \left( \left( \omega_{n/2}^{i} \right)^2 \right) +
    A^{[1]} \left( \left( \omega_{n/2}^{i} \right)^2 \right)
    & \text{per il lemma di Dimezzamento}
    \\
    &= A^{[0]} \left( \omega_{n/2}^{i} \right) +
    A^{[1]} \left( \omega_{n/2}^{i} \right)
    \\
    &= y_i^{[0]} + \omega_{n}^{i} y_i^{[1]} 
\end{align*}
quindi i primi $n/2$ valori vengono trovati con semplici combinazioni dei valori ottenuti per ricorsione. Cerchiamo gli altri $n/2$ valori, sia nuovamente $0 \leq i \leq \frac{n}{2}-1$
\begin{align*}
    y_{i+\frac{n}{2}} &= A \left( \omega_{n}^{i} \right) 
    & \text{per l'identità polinomiale}
    \\
    &= A^{[0]} \left( \left( \omega_{n/2}^{i+n/2} \right)^2 \right) +
    \omega_{n}^{i+n/2} \,
    A^{[1]} \left( \left( \omega_{n/2}^{i+n/2} \right)^2 \right)
    & \text{per il lemma di Dimezzamento}
    \\
    &= A^{[0]} \left( \omega_{n/2}^{i} \right) +
    \omega_{n}^{i} \,
    \omega_{n}^{n/2} \,
    A^{[1]} \left( \omega_{n/2}^{i} \right)
    & \omega_{n}^{n/2} = -1
    \\
    &= y_i^{[0]} - \omega_{n}^{i} \, y_i^{[1]} 
\end{align*}
Si possono quindi calcolare tutti i punti di $A$ con quelli di $ A^{[0]} $ e $ A^{[1]}$

\section{Trasformata veloce di \textit{Fourier}}

\subsection{Algoritmo della trasformata veloce di \textit{Fourier}}

Sfuttando la proprietà di sottostruttura sviluppata nella sezione precedente, si può scrivere l'algoritmo che implementa la trasformata veloce di \textit{Fourier}
\begin{algorithm}[H]
\caption{Trasformata veloce di \textit{Fourier}}\label{alg:fft}
\begin{algorithmic}[1]
    \Procedure{FFT}{$\vec{a}$}
        \State $n \gets \vec{a}.length$
        \If{$n=1$}
        \Comment{BASE}
            \State return $\vec{a}$
        \EndIf
        \State $\vec{a}^{[0]}=\left( a_0, a_2,\cdots, a _{n-2} \right)$
        \label{alg:fft:a0}
        \Comment{DIVIDE}
        \State $\vec{a}^{[1]}=\left( a_1, a_3,\cdots, a _{n-1} \right)$
        \State $\vec{y}^{[0]} \gets \Call{FFT}{\vec{a}^{[0]}}$
        \Comment{RECURSE}
        \State $\vec{y}^{[1]} \gets \Call{FFT}{\vec{a}^{[1]}}$
        \State $ON \gets e^{i 2 \pi/n}$   
        \Comment{CONQUER}
        \State $ONI \gets 1$   
        \label{alg:fft:oni}
        \For{$i \gets 0 $ to $ n/2-1 $ }                   
            \State $y_{i} \gets y_{i}^{[0]} + ONI \cdot y_i^{[1]}$
            \label{alg:fft:con}
            \State $y_{i+n/2} \gets y_{i}^{[0]} - ONI \cdot y_i^{[1]}$
            \State $ONI \gets ONI \cdot ON$
        \EndFor
        \State return $\vec{y}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Note: il $Divide$ può essere implementato senza utilizzare spazio aggiuntivo. Alla riga~\ref{alg:fft:oni} viene salvato il valore iniziale di $\omega_n^i$, che verrà poi incrementato nel corso dell'algoritmo. Alla riga~\ref{alg:fft:con} è presente un'inefficienza, ma un compilatore degno ottimizzerebbe automaticamente il codice, salvando il risultato del prodotto in un temporaneo.

La correttezza dell'algoritmo è provata facilmente per la base, e assumendo di avere a disposizione i valori corretti per $n/2$, il \textit{Conquer} implementa la proprietà di sottostruttura e la corretteza segue immediatamente.

L'algoritmo effettua $2$ chiamate ricorsive su istanze di taglia $n/2$ e $5$ operazioni di somma e prodotto su istanze di taglia $n/2$ e l'equazione del \textit{Master Theorem} risulta
\begin{equation*}
    T_{FFT} (n) = 2 T_{FFT} \left( \frac{n}{2} \right) + 5 \frac{n}{2} %\Theta \left( n \right)
\end{equation*}
da cui ricaviamo la funzione di soglia $n^{\log_2 2} = n$, $w(n)= \Theta \left( n \right)$ e quindi $n = \Theta \left( w\left( n \right) \right)$ per cui siamo nel caso $2$, dove è stato ottenuto un perfetto bilanciamento di soglia e lavoro. La complessità risulta
\begin{equation*}
    T_{FFT} (n) = \Theta \left( n \log n \right)
\end{equation*}

\subsection{Trasformata inversa}
Anche per l'interpolazione esiste un algoritmo veloce, ricordiamo che si può riscivere la trasformata discreta come operatore lineare 
$$\vec{y} = DFT_n \left( \vec{a} \right) = F_n \vec{a}$$
dove $F_n$ è una matrice di \textit{Vandermonde} della forma
\begin{equation*}
    F_n = V\left( \omega_n^0, \cdots, \omega_n^i, \cdots \right) =
    \left(
        \begin{array}[h]{ccccc}
            1 & \cdots & & \cdots & 1 \\
            \vdots \\
            & & \omega_n^{ij} \\
            \vdots \\
            1\\
        \end{array}
    \right)
\end{equation*}
La trasformata inversa si può scrivere come 
$$\vec{a} = DFT_n^{-1} \left( \vec{y} \right) = F_n^{-1}\vec{y}$$
dimostriamo che gli elementi di $F_n^{-1}$ sono 
\begin{equation*}
    \left( F_N^{-1} \right)_{ij} = \frac{1}{n} \omega_n^{-ij}
\end{equation*}
dimostrando che
\begin{equation*}
    F_n F_n^{-1} = F_n^{-1} F_n = I_n
\end{equation*}
nel secondo caso, scriviamo il generico prodotto righe per colonne dell'elemento $ij$
\begin{equation*}
    \left( F_n^{-1} F_n \right)_{ij} =
    \frac{1}{n} \sum_{k=0}^{n-1} \omega_n^{-ik} \omega_n^{kj} =
    \frac{1}{n} \sum_{k=0}^{n-1} \omega_n^{(j-i)k}
\end{equation*}
sulla diagonale principale $i=j$ deve valere $1$ e infatti
\begin{equation*}
    \frac{1}{n} \sum_{k=0}^{n-1} \omega_n^{0} =
    \frac{n}{n} = 1
\end{equation*}
negli altri casi, con $i \neq j$ osserviamo che $j-i$ può assumere solo valori maggiori di $-(n-1)$ quando $j=0, \; i=n-1$ e minori di $n-1$ quando $j=n-1, \; i=0$
\begin{equation*}
    -(n-1) \leq j-i \leq n-1
    \quad \text{con }j-i \neq 0 
    \text{ considerato nell'altro caso}
\end{equation*}
e per il lemma di somma
\begin{equation*}
    \frac{1}{n} \sum_{k=0}^{n-1} \left( \omega_n^{j-i} \right)^k = 0
\end{equation*}
dato che l'esponente non è multiplo di $n$. La prima uguaglianza si dimostra in modo analogo.

Possiamo interpretare $\vec{y} \equiv Y(x)$ come rappresentazione per coefficienti di $Y$ e quindi
\begin{equation*}
\left( F_n^{-1} \vec{y} \, \right)_i = \frac{1}{n} Y \left( \omega_n^{-1} \right)
\end{equation*}
vedendo l'interpolazione come una valutazione scalata di $Y$ in $\left( \omega_n^{-1} \right)$, avendo interpretato come una matrice di \textit{Vandermonde} $F_n^{-1}$
\begin{equation*}
    F_n^{-1} = \frac{1}{n} V \left( \omega_n^{-0}, \omega_n^{-1}, \cdots, \omega_n^{-\left( n-1 \right)} \right)
\end{equation*}
% in più definendo $\bar{\omega}_n$
In più definendo $\overline{\omega}_n = \omega_n^{-1}$
% in più definendo $\overline{\omega_n}$
(da cui $\overline{\omega}_n^{\,2} = \omega_n^{-2}$ e così via fino a $\overline{\omega}_n^{\,n-1} = \omega_n$)
rispetto a $\overline{\omega}_n$ si applicano tutti i lemmi e la proprietà di sottostruttura è valida. Si possono quindi ottenere le valutazioni di un polinomio $Y(x)$ su $\left( \left( \omega_n^{-1} \right)^{0}, \omega_n^{-1}, \cdots \left( \omega_n^{-1} \right)^{n-1}, \right)$ utilizzando un algoritmo identico a $FFT$, $\overline{FFT}$, dove l'unica differenza nel codice è $ON \gets e^{-i2\pi/n} = \omega_n^{-1}$ che è appunto corretto perché rispetto a $\omega_n^{-1}$ vale la proprietà di sottostruttura. Questo algoritmo \emph{non} implementa la trasformata inversa, occorre ancora dividere per $1/n$. Supponendo $\overline{FFT} \left( \vec{y} \right)$ sia noto, si ricava $DFT_n^{-1} \left( \vec{y} \right)$

\begin{algorithm}[H]
\caption{Trasformata inversa}\label{alg:invfft}
\begin{algorithmic}[1]
    \Procedure{INV\_FFT}{$\vec{y}$}
        \State $n \gets \vec{y}.length$
        \State $\vec{a} \gets \overline{FFT} \left( \vec{y} \right)$
        \Comment{algoritmo ausiliario}
        \For{$i \gets 0 $ to $ n-1 $ }
            % \State $a_i \gets \frac{1}{n}a_i$
            \State $a_i \gets a_i / n$
        \EndFor
        \State return $\vec{a}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Nota: vale $T_{INV\_FFT}(n) = \Theta \left( n \log n \right)$

L'algoritmo $INV\_FFT$ \emph{non} è ricorsivo, $\overline{FFT}$ lo è.

Si può ottenere l'algoritmo per $DFT_n^{-1} \left( \vec{y} \right)$ in un altro modo. Notiamo che 
$\omega_n^{-0} = \omega_n^{0} = 1$ e $\forall 1 \leq j \leq n-1$ vale
$\omega_n^{-j} = \omega_n^{-j \bmod n} = \omega_n^{n-j}$
quindi possiamo scrivere
\begin{equation*}
    Y \left( \omega_n^{-j} \right) =
        \begin{cases}
            Y\left( \omega_n^{0} \right) & j=0 \\
            Y \left( \omega_n^{n-j} \right) & j \neq 0
        \end{cases}
\end{equation*}
e le componenti sono quindi quelle di \textit{Fourier} positive ma in ordine diverso e le valutazioni so potenze negative sono le positive permutate.
\begin{equation*}
    \vec{a} = DFT_n^{-1} \left( \vec{y} \right) \quad \rightarrow \quad
    \begin{cases}
        a_0 = \frac{1}{n} Y\left( \omega_n^{0} \right) = 
        \frac{1}{n} \left[ DFT_n \left( \vec{y} \right) \right]_0
        & j=0
        \\
        a_j = \frac{1}{n} Y\left( \omega_n^{-j} \right) = 
        \frac{1}{n} \left[ DFT_n \left( \vec{y} \right) \right]_{n-j}
        & j>0
    \end{cases}
\end{equation*}

\begin{algorithm}[H]
\caption{Trasformata inversa}\label{alg:invfftbis}
\begin{algorithmic}[1]
    \Procedure{INV\_FFT}{$\vec{y}$}
        \State $n \gets \vec{y}.length$
        \State $\vec{z} \gets FFT \left( \vec{y} \right)$
        \State $a_0 \gets z_0 / n$
        \For{ $j \gets 1 $ to $ n-1 $ }
            \State $a_j \gets z_{n-j} / n $
        \EndFor
        \State return $\vec{a}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Introducendo l'operatore vettoriale $\cdot^{rev}$ definito come
\begin{equation*}
    \vec{a} = \left( a_0, a_1, \cdots, a_{n-1} \right)
    \quad \rightarrow \quad
    \vec{a}^{rev} = \left( a_0, a_{n-1}, \cdots, a_1 \right)
\end{equation*}
si può riscrivere la trasformata inversa come
\begin{equation*}
    DFT_{n}^{-1} \left( \vec{y} \, \right) = \frac{1}{n} \left( DFT_{n} \left( \vec{y} \, \right) \right)
\end{equation*}

\section{Convoluzione lineare}

Utilizzando l'algoritmo della trasformata veloce, si può scrivere un algoritmo efficiente per la convoluzione lineare
\begin{equation*}
    \vec{c} = \vec{a} * \vec{b}
    \quad
    \vec{a}, \vec{b} \in \mathbb{C}^n, \vec{c} \in \mathbb{C}^{2n-1}
\end{equation*}
supponendo entrambi i vettori di lunghezza uguale e potenza di due.

\begin{algorithm}[H]
\caption{Convoluzione lineare}\label{alg:convlin}
\begin{algorithmic}[1]
    \Procedure{LIN\_CONV}{$\vec{a}, \vec{b}$}
        \State $n \gets \vec{a}.length$
        \State $\vec{a}\,' \gets \left( \vec{a} \, | 0_n \right)$
        \Comment{$\vec{a}\,', \vec{b}\,' \in \mathbb{C}^{2n} $ }
        % \State $\vec{a}' \gets \left( \vec{a} \, | 0_n \right)$
        \State $\vec{b}\,' \gets \left( \vec{b} \, | 0_n \right)$
        \State $\vec{y}_a \gets FFT \left( \vec{a}\,' \right)$
        \Comment{valutazioni di $A(x)$ e $B(x)$ estese su $2n$ punti}
        \State $\vec{y}_b \gets FFT \left( \vec{b}\,' \right)$
        \For{ $i \gets 0 $ to $ 2n-1 $ }
        \State $ \left( y_c \right)_i = \left( y_a \right)_i \cdot \left( y_b \right)_i $
        \EndFor
        \State return \Call{INV\_FFT}{$\vec{y}_c$}
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Dove il polinomio prodotto $\vec{c} = \text{INV\_FFT}\left( \vec{y}_c \right) \in \mathbb{C}^{2n}$ e vale sempre $c_{2n-1}=0$

Vengono eseguite $3$ trasformate su vettori di $2n$ elementi, per cui
\begin{equation*}
    T_{LC} (n) = \Theta \left( n \log n \right)
\end{equation*}

Si può quindi enunciare il seguente teorema della convoluzione lineare

\begin{theorem}[Convoluzione lineare]
    Se $\vec{c} = \vec{a} * \vec{b}$ allora
    \begin{equation*}
        \vec{c} = DFT_{2n}^{-1} \left( 
            DFT_{2n} \left( \vec{a} \, | 0_n \right)
            \odot
            DFT_{2n} \left( \vec{b} \, | 0_n \right)
        \right)
    \end{equation*}
    \label{teo:convlin}
\end{theorem}

\section{Trasformate notevoli}

\subsection{Vettore costante}
Calcoliamo il valore di $DFT_n \left( a, a, \cdots, a \right) = F_n \vec{a}$, con $F_n$ definita all'equazione \ref{eq:matricefourier}.
\begin{equation*}
    F_n
    \left( 
        \begin{array}[h]{c}
            a \\ a \\ \vdots \\ a
        \end{array}
    \right)
    =
    a F_n
    \left( 
        \begin{array}[h]{c}
            1 \\ 1 \\ \vdots \\ 1
        \end{array}
    \right)
    \overset{\circola{1}}{=}
    a
    \left( 
        \begin{array}[h]{c}
            \sum_{j=0}^{n-1} \left( \omega_n^i \right)^j  \\
            \vdots
        \end{array}
    \right)
    \overset{\circola{2}}{=}
    a
    \left( 
        \begin{array}[h]{c}
            n \\ 0 \\ \vdots \\ 0
        \end{array}
    \right)
    =
    \left( 
        \begin{array}[h]{c}
            an \\ 0 \\ \vdots \\ 0
        \end{array}
    \right)
\end{equation*}
Dove in $\circola{1}$ il vettore contiene in ogni componente la somma di tutti gli elemeti sulla stessa riga della matrice di \textit{Fourier} e in $\circola{2}$ si applica il lemma di somma \ref{teo:somma}.

\subsection{Vettore impulsivo}
Calcoliamo il valore di $DFT_n \left( a, 0, \cdots, 0 \right)$
\begin{equation*}
    F_n
    \left( 
        \begin{array}[h]{c}
            a \\ 0 \\ \vdots \\ 0
        \end{array}
    \right)
    =
    a F_n
    \left( 
        \begin{array}[h]{c}
            1 \\ 0 \\ \vdots \\ 0
        \end{array}
    \right)
    \overset{\circola{1}}{=}
    a \left( F_n \right)^1
    =
    a
    \left( 
        \begin{array}[h]{c}
            1 \\ 1 \\ \vdots \\ 1
        \end{array}
    \right)
    =
    \left( 
        \begin{array}[h]{c}
            a \\ a \\ \vdots \\ a
        \end{array}
    \right)
\end{equation*}
Dove in $\circola{1}$ viene selezionata la prima riga di $F_n$, composta tutta di $1$.

\subsection{Vettore $(n,k)$-sparso}
\begin{definition}[Vettore $(n,k)$-sparso]
    Un vettore si dice $(n,k)$-sparso se, dati $1 \leq k \leq n$, con $n,k$ potenze di due, e dato $\vec{x} \in \mathbb{C}^n$ vale
    \begin{equation*}
        x_i = 0 \quad \forall i : i \bmod k \neq 0
    \end{equation*}
    Ossia il vettore ha componenti nulle per gli indici che \emph{non} sono multipli di $k$.
    \label{def:nksparso}
\end{definition}
Il segnale è spaziato in maniera uniforme, diviso in $n/k$ sottocomponenti di lunghezza $k$.
Esempio: $n=8, k=4 \rightarrow \left( x_0,0,0,0,x_4,0,0,0 \right)$

Si può quindi seguire l'algoritmo della $FFT$ e cercare una sottostruttura più potente. Nel caso base $k=1$ il vettore $\left( n,1 \right)$-sparso risulta un vettore generico $\vec{x}=\left( x_0, x_1,\cdots, x _{n-1} \right) \in \mathbb{C}^n$ quindi non si può fare di meglio di $FFT\left( \vec{x} \right)$. Per il caso generico $k>1$, riscrivendo i due vettori delle componenti pari e dispari possiamo notare come il secondo abbia appunto tutte componenti relative ad indici dispari, che per un vettore sparso sono tutte pari a zero.
\begin{equation*}
    \vec{x}^{[0]}=\left( x_0, x_2,\cdots, x _{n-2} \right)
    \quad 
    \vec{x}^{[1]}=\left( x_1, x_3,\cdots, x _{n-1} \right) = \vec{0}_{n/2}
\end{equation*}
Metà del lavoro ricorsivo è quindi eliminato.
Per essere una proprietà di sottostruttura valida, per qualche $k'$ anche $\vec{x}^{[0]} \in \mathbb{C}^{n/2}$ deve essere sparso. Consideriamo $0 \leq i \leq n/2-1$, vale
\begin{align*}
    \vec{x}_i^{[0]} = x_{2i} \meq 0 
    & \leftrightarrow
    2i \bmod k \neq 0
    \\
    & \leftrightarrow
    \nexists \, m : 2i = mk
    \\
    & \leftrightarrow
    \nexists \, i : m\frac{k}{2} = i
    \\
    & \leftrightarrow
    \nexists \, i \bmod \frac{k}{2} \neq 0
\end{align*}
Quindi $\vec{x}_i^{[0]}$ è uguale a zero per ogni $i$ non multiplo di $k/2$, ovvero $\vec{x}_i^{[0]}$ è $\left( \frac{n}{2}, \frac{k}{2} \right)$-sparso.

Per un vettore $\vec{x}$ $(n,k)$-sparso, con $1 \leq k \leq n$ e $n,k$ potenze di $2$, si può riscrivere l'algoritmo della trasformata di \textit{Fourier} come

\begin{algorithm}[H]
\caption{Trasformata veloce di \textit{Fourier} per vettori sparsi}\label{alg:fftsparsa}
\begin{algorithmic}[1]
    \Procedure{SPARSE\_FFT}{$\vec{x}, k$}
        \State $n \gets \vec{a}.length$
        \If{$n=1$}
        \Comment{BASE}
            \State return $\vec{a}$
        \EndIf
        \State $\vec{a}^{[0]}=\left( a_0, a_2,\cdots, a _{n-2} \right)$
        \label{alg:fft:a0}
        \Comment{DIVIDE}
        \State $\vec{a}^{[1]}=\left( a_1, a_3,\cdots, a _{n-1} \right)$
        \State $\vec{y}^{[0]} \gets \Call{FFT}{\vec{a}^{[0]}}$
        \Comment{RECURSE}
        \State $\vec{y}^{[1]} \gets \Call{FFT}{\vec{a}^{[1]}}$
        \State $ON \gets e^{i 2 \pi/n}$   
        \Comment{CONQUER}
        \State $ONI \gets 1$   
        \label{alg:fft:oni}
        \For{$i \gets 0 $ to $ n/2-1 $ }                   
            \State $y_{i} \gets y_{i}^{[0]} + ONI \cdot y_i^{[1]}$
            \label{alg:fft:con}
            \State $y_{i+n/2} \gets y_{i}^{[0]} - ONI \cdot y_i^{[1]}$
            \State $ONI \gets ONI \cdot ON$
        \EndFor
        \State return $\vec{y}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{autocomplete}
Una bella scatola:
\begin{equation}
    \boxed{x^2+y^2 = z^2}
\end{equation}

Numeri nei casi
\begin{numcases}{T(n)=}
    2^3 \label{escaso1} \\
    2^4 \label{escaso2} 
\end{numcases}

Sotto numeri
\begin{subnumcases}{T(n)=}
    2^3 \label{escaso3} \\
    2^4 
\end{subnumcases}

\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item qualcosa
    \item[+] qualcosa
    \item[*] qualcosa
    \item[--] qualcosa
\end{itemize}
àg
èg
ìg
òg
ùg
perché

delirio di vim se scrivi \verb|<C-k>`e| o \verb|<C-k>e`| in insert mode mette una è

% delirio doppio di vim se scrivi \verb|<C-k>da| in insert mode mette ``Hiragana letter DA'' che purtroppo non posso mostrarvi %だ

Spazietti fra equazioni
\begin{equation*}
    A^{[0]}(x) = \sum_{j=0}^{\frac{n}{2}-1} a_{2j}x^j
    \quad \text{ e } \quad
    A^{[1]}(x) = \sum_{j=0}^{\frac{n}{2}-1} a_{2j+1}x^j
\end{equation*}

Un gustoso algoritmo
\begin{algorithm}[H]
\caption{Divide and Conquer}\label{alg:dncmock}
\begin{algorithmic}[1]
    \Procedure{D\&C}{$i$}
        \If{$|i| \leq n_0$}                             \Comment{BASE}
            \State *risolvo direttamente*
        \EndIf
        \State $<i_1, i_2, \dots, i_k> \gets A_D(i)$    \Comment{DIVIDE}
        \For{$j \gets 1 $ to $ k $ }                    \Comment{RECURSE}
            \State $s_j \gets $ \Call{D\&C}{$i_j$}
        \EndFor
        \State $s \gets A_C(<s_1, s_2, \dots, s_k>)$    \Comment{CONQUER}
        \State return $s$
    \EndProcedure
\end{algorithmic}
\end{algorithm}


