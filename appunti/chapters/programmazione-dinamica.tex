\section{Paradigma del \emph{Dynamic Programming}}

\subsection{Introduzione}

% TODO disegno albero TD BU pag 53.2
Il problema del memorylessness del paradigma \emph{Divide and Conquer} è già stato sottolineato nella sezione \ref{sss:alberochiamate}.
Per esemplificare il problema, si introduce la sequnza di Fibonacci, definita come
\begin{equation*}
    F_n = 
    \begin{cases}
        1 & n=0,1 \\
        F_{n-1} + F_{n-2} & n>1
    \end{cases}
\end{equation*}
da cui si ricava immediatamente un algoritmo ricorsivo
\begin{algorithm}[H]
\caption{Fibonacci ricorsivo}\label{alg:rfib}
\begin{algorithmic}[1]
    \Procedure{R\_FIB}{$n$}
        \If{$ n=0 $ or $n=1$}
            \State return $1$
        \EndIf
        \State return \Call{R\_FIB}{$n-1$} + \Call{R\_FIB}{$n-2$}
    \EndProcedure
\end{algorithmic}
\end{algorithm}
% TODO disegno albero chiamate R_FIB pag 53.4
\noindent
la cui equazione di ricorrenza è
\begin{equation*}
    T_{RF}(n) = 
    \begin{cases}
        0 & n=0,1 \\
        T_{RF}(n-1) + T_{RF}(n-2) + 1 & n>1
    \end{cases}
\end{equation*}
che risulta limitata inferiormente da un esponenziale, infatti per $n>1$
\begin{align*}
    T_{RF}(n) 
    &= T_{RF}(n-1) + T_{RF}(n-2) + 1 \\
    & \geq \, 2 T_{RF}(n-2) + 1 \\
    & \geq \, 2^2 T_{RF}(n-2-2) + 2 + 1 \\
    & \geq \, 2^i T_{RF}(n-2i) + \sum_{j=0}^{i-1} 2^j \\
    \intertext{che raggiunge il caso base quando $n-2i=0$ o $n-2i=1$ per $i= \left\lfloor n/2 \right\rfloor $ sia nel caso di $n$ pari sia nel caso di $n$ dispari}
    & \geq \, 2^{\left\lfloor n/2 \right\rfloor} \cancel{ T_{RF}(0 \text{ o } 1)}
    + \sum_{j=0}^{\left\lfloor n/2 \right\rfloor -1} 2^j \\
    &= 2^{\left\lfloor n/2 \right\rfloor} -1 \\
    &=  \Omega \left( 2^{n/2} \right) = \sqrt{2}^{\,n}
    \intertext{ed essendo $\sqrt{2}>1$ cresce più velocemente di ogni polinomio. Il valore esatto di $T_{RF}$ è}
    T_{RF}(n) &= \Theta \left( \left( \frac{1+\sqrt{5}}{2} \right)^n \right)
\end{align*}

Il sugo della storia è che viene calcolata un numero molto elevato di volte la stessa sottoistanza. Ispirandosi alla fase \emph{Bottom-Up} del \emph{D\&C}, e sfruttando strutture dati che contengano le informazioni necessarie, si può scrivere un algoritmo iterativo che risolve il problema.
\begin{algorithm}[H]
\caption{Fibonacci iterativo}\label{alg:itfib}
\begin{algorithmic}[1]
    \Procedure{IT\_FIB}{$n$}
        \If{$ n=0 $ or $n=1$}
            \State return $1$
        \EndIf
        \State $F[0] \gets 1$
        \State $F[1] \gets 1$
        \For{$i \gets 2 $ to $ n $ } 
            \State $F[i] \gets F[i-1] + F[i-2]$
        \EndFor
        \State return $F[n]$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
dove le soluzioni intermedie sono salvate in $F$. In realtà sarebbe sufficiente memorizzare solo gli ultimi due valori della sequenza.

Si può quindi introdurre il paradigma del \emph{Dynamic Programming}, basandosi su queste osservazioni.

\subsection{Paradigma del \emph{Dynamic Programming}}
% \subsection{Introduzione}
I due concetti fondamentali su cui si basa il paradigma \emph{Dynamic Programming} sono
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item[--] dotare di memoria l'algoritmo
    \item[--] implementare la computazione in direzione \emph{bottom-up} (tutti i dati necessari sono già stati calcolati nelle iterazioni precedenti)
\end{itemize}
In ogni caso si lavora con la proprietà di sottostruttura, generando la soluzione ad un'istanza in funzione di sottoistanze di taglia minore.

Il vantaggi sono una maggiore velocità dovuta al non replicare la computazione, gli svantaggi sono legati al dover implementare la computazione in maniera da essere sicuri di avere a disposizione le soluzioni necessarie al momento giusto, che per casi articolati non è triviale. Si può sfruttare la convenienza della fase \emph{top-down}, che genera e risolve le istanze in ordine coerente, scrivendo l'algoritmo in maniera ricorsiva e dotandolo di memoria, come descritto nella sezione seguente.

\section{Memoizzazione di un algoritmo ricorsivo}

È possibile modificare un algoritmo ricorsivo \emph{D\&C} memorizzando le soluzioni intermedie attraverso un processo detto di memoizzazione.

\subsection{Metodo generale}

Un algoritmo memoizzato è costituito da due subroutine:
\begin{enumerate}
    \item \textbf{Routine di inizializzazione} INIT\_\{AlgName\}
        \begin{itemize}
            \item risolve i casi di base direttamente
            \item inizializza una struttura tabellare \emph{globale} con
                \begin{itemize}
                    \item valori delle istanze di base, nelle locazioni associate alle istanze di base
                    \item valori di default in posizioni associate a istanze non di base (il valore di default deve essere scelto in modo da far capire che non è stato ancora calcolato)
                \end{itemize}
            \item invoca la seconda procedura (ricorsiva) \\ % FORMATTA
        \end{itemize}
    \item \textbf{Routine ricorsiva} REC\_\{AlgName\}($i$)
        \begin{itemize}
            \item controlla sulla tabella per vedere se l'istanza $i$ è già stata risolta
                \begin{itemize}
                    \item se sì la ritorna
                    \item se no
                        \begin{itemize}
                            \item la calcola con la proprietà di sottostruttura
                            \item la memorizza nella tabella
                            \item la ritorna
                        \end{itemize}
                \end{itemize}
        \end{itemize}
\end{enumerate}

\textbf{Nota:} lo spazio delle sottoistanze deve essere 
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item[--] piccolo
    \item[--] facilmente indicizzabile
\end{itemize}

\subsection{Algoritmo di Fibonacci memoizzato}

Per l'algoritmo di Fibonacci le due funzioni risultano
\begin{algorithm}[H]
\caption{Fibonacci memoizzato}\label{alg:fibmemoizzato}
\begin{algorithmic}[1]
    \Procedure{INIT\_FIB}{$n$}
        \If{$ n=0 $ or $n=1$}
            \State return $1$
        \EndIf
        \State $F[0] \gets 1$, $F[1] \gets 1$
        % \State $F[1] \gets 1$
        \For{$i \gets 2 $ to $ n $ } 
            \State $F[i] \gets 0$
        \EndFor
        \State return \Call{REC\_FIB}{$n$}
    \EndProcedure
    \Procedure{REC\_FIB}{$n$}
        \If{$F[i] = 0$}
            \State $F[i] \gets \Call{REC\_FIB}{i-1} + \Call{REC\_FIB}{i-2} $
        \EndIf
        \State return $F[i]$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\textbf{Note:}
$n$ è un parametro attuale, il valore che si vuole calcolare, mentre $i$ è un parametro formale, che descrive la generica sottoistanza su cui lavora l'algoritmo ricorsivo.
Nella funzione ricorsiva si testa se la struttura tabellare contiene già la soluzione della sottoistanza, e se no lo si calcola con l'algoritmo ricorsivo e lo si salva. Nelle successive chiamate il test fallisce e non è necessario calcolare nuovamente il valore.
Il \emph{caveat} nel dare memoria a un algoritmo \emph{D\&C} è che in lo spazio delle sottoistanze deve essere molto piccolo, in questo caso una sola sottoistanza per ogni valore di $n$. Inoltre perché l'algoritmo sia efficiente lo spazio delle possibili sottoistanze da risolvere deve essere facilmente indirizzabile e salvabile in una struttura dati semplice.

\textbf{Analisi della complessità:}
La ricorrenza non riesce a catturare il fatto che l'albero delle chiamate venga tagliato ogniqualvolta il valore di una sottoistanza sia stato calcolato precedentemente.
% TODO albero delle chiamate memoizzato pag 55.9
Per la fase di inizializzazione, il numero di operazioni aritmetiche che vengono eseguite è nullo.
Dall'albero delle chiamate della procedura ricorsiva con memoria, si nota che l'albero è diventato molto più snello, e sono comparse foglie dove prima erano presenti sottoalberi. Inoltre il numero di nodi interni, gli unici rispetto a cui viene fatto lavoro, sono pari al numero di sottoistanze \emph{distinte} che vanno risolte per ottenere l'istanza $n$. 
Solo nel nodo interno si esegue il conquer (la somma) associato ad ogni chiamata e la complessità sarà corrispondente al numero di nodi interni per il lavoro compiuto in ciascun nodo. C'è un nodo interno per ogni chiamata \emph{non} di base, tutte le altre sono chiamate che hanno trovato il valore in $F$ e non hanno compiuto lavoro, se non un \emph{lookup} nella tabella.
\begin{equation*}
    T_{RF} (n) = n-2+1 = n-1
\end{equation*}

\section{Paradigma generale \emph{Dynamic Programming}}

\subsection{Problemi di ottimizzazione combinatoria}

Ricordiamo la definizione di problema computazionale $\bpi{} \subseteq \bi{} \times \bs{}$

Si può definire per ogni istanza un sottoinsieme $\bs{}(i)$ formato da tutte le possibili soluzioni di quell'istanza, ricordando che una sottoistanza può ammetere più soluzioni.
\begin{equation*}
    \forall i \in \bs{} \quad S(i) = \left\{ s \in \bs{} : i \, \bpi{} \, s \right\}
\end{equation*}

Definiamo inoltre una funzione di costo che va da S in un qualche insieme totalmente ordinato.
\begin{equation*}
    c : \bs{} \to \mathbb{R}
\end{equation*}

Data $i$ un istanza generica, si vuole determinare non solo una soluzione $s \in \bs{}(i)$, ma individuare $s^*$ che massimizza (o minimizza) il criterio del costo.
\begin{equation*}
    c(s^*) = \max \left\{ c(s) : s \in \bs{}(i) \right\}
\end{equation*}

\subsection{Caratteristiche di problemi di ottimizzazione risolubili con la programmazione dinamica}

Un problema si presta ad essere risolto con la programamzione dinamica se presenta la seguente proprietà di sottostruttura ottima, che per un prolbema di ottimizzazione è una proprietà molto più ristretta rispetto a quella necessaria per il \emph{D\&C}. Infatti mette in relazione soluzioni ottime di un istanza a soluzioni ottime di sottoistanze. Questa proprietà si dice anche \emph{optimal substructure property}.

% Dare paradigmi generali che si applicano quando un problema va approcciato con la programamzione dinamica.

% Come nel dnc si sviluppa una proprietà di sottostruttura, possiamo seguire una serie di indicazioni generali che si possono istanziare volta per volta che conducono in maniera ordinata alla risuluzone di un problema con la prog dinanica. 

% Quando si decide di usare la prog dinamica?

\begin{definition}[Proprietà di sottostruttura ottima]
    Un problema gode della proprietà di sottostruttura ottima se
    \begin{enumerate}
        \item la soluzione ottima di un'istanza non di base si ottiene combinando soluzioni ottime di sottoistanze
        \item la proprietà di sottostruttura ottima genera sottoproblemi ripetuti
        \item lo spazio delle sottoistanze generate da una data istanza non è troppo grande
    \end{enumerate}
\end{definition}

Se la proprietà 2 manca, si può applicare il \emph{D\&C} classico e la programamzione dinamica non aiuterebbe.
La proprietà 3 assicura l'efficienza del paradigma, il concetto è arbitrario per ora, se non esiste una struttura dati che riesca a memorizzare in maniera efficiente i risultati intermedi, il metodo avrà risultati peggiori. In genere la dimensionalità della struttura è pari al numero di sottoistanze generate (e.g. se istanze di taglia $|i|=n$ generano $n^2$ sottoistanze, possono essere salvate in un array \emph{bi}dimensionale, $n^3$ \emph{tri}dimensionale e così via).

\subsection{Paradigma generale}

Presentiamo un paradigma generale per la programmazione dinamica, enumerando i passi da seguire per sviluppare un programma secondo il \emph{dynamic programming}, garantendo implicitamente la correttezza del codice.

\begin{enumerate}
    \item caratterizza la struttura di una soluzione ottima $s^*$ a un'istanza $i$ non di base in funzione di soluzioni ottime $s_1^*, s_2^*, \cdots, s_k^*$ di sottoistanze di $i$
    \item
        \begin{enumerate}
            \item determina una relazione di ricorrenza sui costi di istanza e sottoistanza, mettendo in relazione il costo di un'istanza come una funzione dei costi delle sottoistanze \\ $c(s^*)=f\left(c(s_1^*),c(s_2^*),\cdots,c(s_k^*)\right)$
                \label{enum:pd1}
            \item determina la minima informazione strutturale necessaria ad ottenere $s^*$ a partire da $s_1^*, s_2^*, \cdots, s_k^*$, cercando di memorizzare incrementi, e non le intere soluzioni di sottoistanze
                \label{enum:pd2}
        \end{enumerate}
    \item
        \begin{enumerate}
            \item calcola il costo $c(s^*)$ utilizzando la ricorrenza impostando la computazione
                \begin{itemize}
                    \item in maniera \emph{bottom-up} iterativa
                    \item in maniera memoizzata
                \end{itemize}
                \label{enum:pd3}
            \item calcola l'informazione addizionale strutturale per ottenere $s^*$ e memorizzarla
                \label{enum:pd4}
        \end{enumerate}
\end{enumerate}

I punti \ref{enum:pd1}, \ref{enum:pd3} sono sufficienti per ricavare il costo della soluzione ottima,
mentre i punti \ref{enum:pd2}, \ref{enum:pd4} sono necessari quando si è interessati anche al valore della soluzione ottima.

\section{Ricerca della \emph{Longest Common Subsequence}}

\subsection{Sottostringhe e sottosequenze}

\subsubsection{Notazione}

\begin{itemize}
    \item[--] $\Sigma$ un alfabeto finito di simboli
    \item[--] $\Sigma^{*}$ la sua stella di \emph{Kleene}, l'insieme infinito di concatenazioni finite di simboli in $\Sigma$
    \item[--] $X = < x_1, x_2, \cdots, x_m > \in \Sigma^{*}$ una stringa di lunghezza $m = |X|$
    \item[--] $X = \varepsilon$ la stringa vuota
\end{itemize}

\subsubsection{Prefisso, suffisso, sottostringa}

\begin{definition}[Prefisso]
    Data una stringa $X$ con $|X|=m$ 
    un prefisso proprio
    è formato dai primi $i$ simboli di $X$.
    \[
        {X_i = < x_1, \cdots, x_i >}
        \quad \text{con} \quad
        1 \leq i \leq m
    \]
    Il prefisso improprio $X_0$ è definito come la stringa vuota.
    \label{def:prefisso}
\end{definition}

\begin{definition}[Suffisso]
    Data una stringa $X$ con $|X|=m$ 
    un suffisso proprio
    è formato dagli ultimi $j$ simboli di $X$.
    \[
        {X^j = < x_j, \cdots, x_m >}
        \quad \text{con} \quad
        1 \leq j \leq m
    \]
    Il prefisso improprio $X^{m+j}$ è definito come la stringa vuota.
    \label{def:suffisso}
\end{definition}

\begin{definition}[Sottostringa]
    Data una stringa $X$ con $|X|=m$ 
    una sottostringa
    è formata da $j-i+1$ simboli contigui di $X$.
    \[
        X_{i..j} = < x_i, \cdots, x_j >
        \quad \text{con} \quad
        1 \leq i \leq j \leq m
    \]
    Se $i>j$, $X_{i..j}=\varepsilon$
    \label{def:sottostringa}
\end{definition}

Lo spazio delle sottoistanze di una stringa $|X|=m$ risulta
\begin{align*}
    1 + \sum_{i=1}^{m} \sum_{j=i+1}^{m} 1 
    &= 
    1 + \sum_{i=1}^{m} \left( m-i+1 \right)
    &
    i'=m-i+1
    \\
    &=
    1 + \sum_{i'=1}^{m} i' 
    \\
    &=
    1 + \frac{m\left( m+1 \right)}{2}
    \\
    &=
    \Theta \left( m^2 \right)
\end{align*}
dove avendo già considerata una volta la sottostringa impropria, l'indice $i$ può variare a piacimento, mentre l'indice $j$ ne è sempre maggiore.

\subsubsection{Sottosequenza}

\begin{definition}[Sottosequenza]
    Data una stringa $X$ con $|X|=m$,
    $Z = < z_1, \cdots, z_k >$ è una sottosequenza di $X$ se esiste una successione di indici crescente
    % $
    \[
    1 \leq i_1 < i_2 < \cdots < i_k \leq m
    \]
    % $
    tale che 
    $z_j = x_{i_j}$, $1 \leq j \leq k$
    % \\
    , ed è la successione di indici crescente che \emph{realizza} $Z$ in $X$
    \label{def:sottosequenza}
\end{definition}

Per esempio, sia $X = <A,B,C,B,B,D>$, una sua sottostringa è $Z_1 = X_{1..3} = <A,B,C>$ mentre una sua sottosequenza è $Z_2 = <A,C,B>$. La stessa sottosequenza può essere realizzata da più successioni di indici, infatti: $i_1=1, \, i_2=3, \, i_3=\left\{ 4,5 \right\}$

Una sottostringa è sempre anche una sottosequenza, infatti $X_{k..s}$ è realizzata da $i_1=k, \, i_2=k+1, \, \cdots, \, i_{s-k+1}=s$

Lo spazio delle sottoistanze è considerevolmente più ampio, infatti il numero di possibili sottoinsiemi ordinati di indici $S \subseteq \left\{ 1, \cdots, m \right\}$, che quindi generano una sottosequenza valida, è pari a $2^m$.

\subsection{Definizione del problema}

Si può quindi introdurre il problema della ricerca della sottosequenza comune \emph{più lunga} tra due stringhe. Formalmente:

\begin{definition}
    Data un'istanza $(X,Y) \in \Sigma^* \times \Sigma^*$, con
    $ X = < x_1, \cdots, x_m > $ e 
    $ Y = < y_1, \cdots, y_n > $,
    allora
    $ Z = < z_1, \cdots, z_k > $
    è sottosequenza comune di $X$ e $Y$ se $Z$ è sottosequenza di entrambe le stringhe.
    L'obiettivo è determinare la sottosequenza comune di massima lunghezza 
    $Z^* = LCS\left( X,Y \right)$
    \label{def:lcs}
\end{definition}
Per provare che una sottosequenza sia comune è sufficiente esibire le due successioni di indici che realizzano $Z$ in $X$ e $Y$, che sia la sottosequenza di lunghezza massima viene assicurato dall'algoritmo che risolve il problema.

Spesso invece di voler individuare $Z^*$ è sufficiente conoscerne il costo $|Z^*|$, di cui si sa che $|LCS(X,Y)| < \min\left\{ m,n \right\}$.

Si può stabilire se $Z$ è sottosequenza di $X$ in tempo $\Theta \left( |X| \right)$, da cui si ricava un algoritmo naive per la ricerca della $LCS$, generando tutte le possibili sottosequenze della stringa più corta e verificando siano sottosequenze della più lunga. Se $m \leq n$, la complessità risulta $\Theta \left( 2^{m} n \right)$.

\subsection{Proprietà di sottostruttura ottima}

Supponiamo di star analizzando la sottoistanza di taglia $m+n$ relativa alle stringhe
% $ X = < x_1, \cdots, x_m > $, $ Y = < y_1, \cdots, y_n > $, e sia
\[
    X = < x_1, \cdots, x_m > \quad  Y = < y_1, \cdots, y_n >
\]
 e sia
$ Z^* = < z_1, \cdots, z_k > $
la soluzione ottima associata a questa sottoistanza. 
Soffermiamoci sull'ultimo elemento delle due stringhe:
% $ X = < X_{m-1}, a > $ e $ Y = < Y_{n-1}, b > $.
\[
     X = < X_{m-1}, a >  \quad  Y = < Y_{n-1}, b >
\]
Si possono presentare due casi.
\begin{enumerate}
    \item $a=b$: di sicuro $z_k=a=b$ e $ Z^* = < Z_{k-1}^*, a > $: intuitivamente se $Z^*$ non terminasse con $a$, esisterebbe $\hat{Z}$ di lunghezza massima che termina con $\hat{z}_k \neq a$ ed è sottosequenza di $X_{m-1}^*$ e $Y_{n-1}^*$ (non terminando con $a$), ma appendendo $a$ a $\hat{Z}$ si otterrebbe una sottosequenza più lunga, che è assurdo.
        \\
        Inoltre $Z_{k-1}^*$ sta in $X_{m-1}^*$ e $Y_{n-1}^*$, avendo già considerato l'ultimo elemento di entrambe.
        \\
        La sottoistanza generata ha taglia $m+n-2$
    \item $a \neq b$: di sicuro $z_k$ non può essere uguale ad $a$ e $b$ contemporaneamente (e può comunque essere diverso da entrambi i valori). Deve quindi verificarsi uno dei due sottocasi:
        \begin{enumerate}[label=(\roman*)]
            \item $z_k \neq a$: va risolto $LCS\left( X_{m-1}, Y \right)$
            \item $z_k \neq b$: va risolto $LCS\left( X, Y_{n-1} \right)$
        \end{enumerate}
        In entrambi i casi la sottoistanza generata ha taglia $m+n-1$
\end{enumerate}

Lo spazio dei sottoproblemi è $\left\{ \left( X_i, Y_j \right), \: 0 \leq i \leq m, \: 0 \leq j \leq n \right\}$ ed ha quindi taglia pari a $(m+1)(n+1) = \Theta (mn)$

Formalizzando la proprietà di sottostruttura ottima risulta:

\begin{lemma}[Proprietà di sottostruttura ottima per la \emph{Longest Common Subsequence}]
    Per un generico sottoproblema $\left( X_i, Y_j \right), \: 0 \leq i \leq m, \: 0 \leq j \leq n$ sia 
    $ Z^* = < z_1, \cdots, z_k > = LCS\left( X_i,Y_j \right)$ allora:
    \begin{enumerate}
        \item $(i=0) \vee (j=0) \Rightarrow z^* = \varepsilon$
            \label{psolcs:casobase}
        \item $(i>0) \wedge (j>0) \wedge (x_i = y_j) \Rightarrow $
            \begin{enumerate}
                \item $z_k = x_i = y_j$
                    \label{psolcs:caso2a}
                \item $Z_{k-1}^* = LCS(X_{i-1}, Y_{j-1})$
                    \label{psolcs:caso2b}
            \end{enumerate}
        \item $(i>0) \wedge (j>0) \wedge (x_i \neq y_j) \Rightarrow $
            \\
            $ Z^*$ è la stringa più lunga tra $LCS\left( X_{i-1}, Y_j \right)$ e $LCS\left( X_i, Y_{j-1} \right)$
            \label{psolcs:caso3}
    \end{enumerate}
    \label{lemma:psolcs}
\end{lemma}

\begin{proof}[Caso \ref{psolcs:casobase}]
    La dimostrazione del caso \ref{psolcs:casobase} è immediata, infatti l'unica sottosequenza comune è la stringa vuota $\varepsilon$.
    \[
        LCS(\varepsilon, Y_j), LCS(X_i, \varepsilon) \Rightarrow Z^*=\varepsilon 
    \]
\end{proof}

\begin{proof}[Caso \ref{psolcs:caso2a}]
    La dimostrazione del caso \ref{psolcs:caso2a} si svolge per assurdo:
    Sia 
    $ Z^* = < z_1, \cdots, z_k > $
    la soluzione ottima realizzata da 
    $1 \leq i_1 < i_2 < \cdots < i_k \leq i $ in $X_i$
    e da
    $1 \leq j_1 < j_2 < \cdots < j_k \leq j $ in $Y_j$
    .\\
    Si supponga per assurdo che
    \[
        z_k \neq x_i ( \neq y_j)
    \]
    Per $1 \leq s \leq k$ il carattere generico $z_s$ è
    \[
        z_s = x_{i_s} = y_{j_s}
    \]
    Applicando l'ipotesi assurda rispetto a  $z_s$ per $s=k$ vale
    \[
        z_k = x_{i_k} \neq x_i \quad z_k = y_{j_k} \neq y_j
    \]
    devono quindi essere diversi gli indici
    \[
        (i_k \neq i) \; \wedge \; (j_k \neq j)
    \]
    e nelle successioni di indici allora
    \[
        (i_k<i \text{ o } i_k \leq i-1) 
        \; \wedge \;
        (j_k<j \text{ o } j_k \leq j-1)
    \]
    per cui $Z^*$ è sottosequenza comune di $X_{i-1}$ e $Y_{j=i}$.
    Si consideri $\hat{Z} = < Z^* , x_i >$, $\hat{Z}$ è sottosequenza sia di $X_i$ sia di $Y_j$
    e le successioni di indici da esibire sono quelle di $Z^*$ con appesi $i_k < i_{k+1} = i$ e $j_k < j_{k+1} = j$.
    Quindi si trova una sottosequenza comune più lunga di $Z^*$, ma $Z^*$ era supposta come soluzione ottima, quindi è stato raggiunto un assurdo.
\end{proof}

\begin{proof}[Caso \ref{psolcs:caso2b}]
    Anche la dimostrazione del caso \ref{psolcs:caso2b} si svolge per assurdo:
    Assumendo che la soluzione ottima $Z^*$ sia realizzata dalle stesse successioni di indici della dimostrazione precedente, gli indici che realizzano
    $Z_{k-1}^* = LCS(X_{m-1}^*, Y_{n-1}^*)$
    sono i primi $k-1$ indici che realizzano $Z^*$.
    \[
        \begin{array}{c|cc}
            1 \leq i_1 < i_2 < \cdots < i_{k-1} & < \cancel{i_k} = i  & \text{ in } X_{i-1}
            \\
            1 \leq j_1 < j_2 < \cdots < j_{k-1} & < \cancel{j_k} = j  & \text{ in } Y_{j-1}
        \end{array}
    \]
    Quindi $i_{k-1} \leq i-1$ e $j_{k-1} \leq j-1$
    per cui 
    $Z_{k-1}^* $ è sottosequenza comune di $X_{i-1}^* $ e $ Y_{j-1}^*$ ed è dunque una soluzione ammissibile; si dimostra anche che è la soluzione ottima.
    \\
    Si supponga per assurdo che
    $Z_{k-1}^* \neq LCS(X_{i-1}^*, Y_{j-1}^*)$
    , allora deve esistere $\bar{Z}$ comune a $X_{i-1}^* $ e $ Y_{j-1}^*$ di lunghezza
    % \[
    $
    | \bar{Z} | > | Z_{k-1}^* | = k-1
    % \]
    $.
    Considerando $\hat{Z} = < \bar{Z} , x_i > $, $\hat{Z}$ è sottosequenza comune di $X_i$ e $Y_j$ di lunghezza
    \[
        | \hat{Z} | = | \bar{Z} | + 1 > k -1 +1 = k = | Z^* |
    \]
    Il che conduce ad un assurdo, essendo $Z^*$ la soluzione ottima.
\end{proof}

\begin{proof}[Caso \ref{psolcs:caso3}]
    Anche la dimostrazione del caso \ref{psolcs:caso3} si svolge per assurdo:
    Sia $ Z^*$ la stringa più lunga tra $LCS\left( X_{i-1}, Y_j \right)$ e $LCS\left( X_i, Y_{j-1} \right)$,
    realizzata da 
    $1 \leq i_1 < i_2 < \cdots < i_k \leq i $ in $X_i$
    e da
    $1 \leq j_1 < j_2 < \cdots < j_k \leq j $ in $Y_j$.
    \\
    Si deve verificare uno dei due casi
    $( z_k \neq x_i ) \vee (z_k \neq y_j)$:
    \\
    Nel primo caso
    $ z_k \neq x_i \Rightarrow i_k<i$
    e $i_k \leq i-1$
    quindi $Z^*$ è in questo caso sottosequenza del prefisso più corto $X_{i-1}$ e come in precedenza è sottosequenza di $Y_j$. È quindi una sottosequenza comune a $X_{i-1}$ e $Y_j$ e deve essere $Z^* = LCS(X_{i-1}, Y_{j})$. Se per assurdo non lo fosse, esisterebbe $\hat{Z}^* = LCS(X_{i-1}, Y_{j})$ con $|\hat{Z}| > |Z^*|$, ma $\hat{Z}$ è anche comune a $X_{i}$ e $Y_j$ che è un assurdo perché $Z^*$ è ottima.
    \\
    Nel secondo caso $Z^* = LCS(X_{i}, Y_{j-1})$, dimostrato analogamente.
    \\
    Dato che uno dei due casi deve succedere, $Z^*$ sarà la più lunga delle due $LCS$ trovate.
\end{proof}
% note su prop strutturali TODO pag 64
La funzione di costo è di due parametri $0 \leq i \leq m$ e $0 \leq j \leq n$
\begin{equation*}
    l(i,j) = 
    \begin{cases}
        0 & (i=0) \vee (j=0)
        \\
        1+l(i-1,j-1) & (i,j>0) \wedge (x_i = y_j)
        \\
        \max \left\{ l(i-1,j) , l(i,j-1) \right\}
        & \text{altrimenti}
    \end{cases}
\end{equation*}
% TODO esempio albero pag 64.5
Il caso peggiore è caratterizzato da $m=n$ quando tutti i caratteri sono diveri tra loro e $LCS=\varepsilon$ e l'albero si biforca sempre.
\begin{equation*}
    T(n,n) = 
    \begin{cases}
        0 & n=0 \\
        T(n-1, n) + T(n, n-1) + 1 & n>0
    \end{cases}
\end{equation*}
Essendo una funzione monotona
\begin{align*}
    T(n,n) &\geq 2T(n-1,n-1)+1
    \intertext{e si può procedere per unfolding:}
    & \geq 2 \cdot 2 T(n-1-1,n-1-1)+2+1
    \\
    \dots & \geq 2^j T(n-j,n-j)+\sum_{k=0}^{j-1}2^k
    \intertext{il caso base si raggiugne per $n-j=0$ ossia $j=n$}
    & \geq \sum_{j=0}^{n-1}2^k = 2^{n-1}
\end{align*}
Seguendo il paradigma \emph{D\&C} anche solo calcolare il costo della souzione ottima è esponenziale.

\subsection{Implementazione dell'algoritmo \emph{LCS} iterativo}
Per applicare il paradigma del \emph{Dynamic Programming}, occorre individuare la minima informazione addizionale per ricostruire la soluzione ottima.
In un vettore bidimensionale si può salvare quale caso è successo, memorizzando uno di tre simboli:
\begin{equation*}
    b(i,j) = 
    \begin{cases}
        ` \nwarrow \textrm{'}
        &
        LCS(X_i, Y_j)
        = < LCS(X_{i-1}, Y_{j-1}), x_i >
        \\
        ` \uparrow \textrm{'}
        &
        LCS(X_i, Y_j)
        = LCS(X_{i}, Y_{j-1})
        \\
        ` \leftarrow \textrm{'} 
        &
        LCS(X_i, Y_j)
        = LCS(X_{i-1}, Y_{j})
    \end{cases}
\end{equation*}

Si può seguire un algoritmo iterativo per calcolare il costo, avendo cura di aver già calcolato i sottoproblemi necessari al momento giusto. Per l'istanza $(i,j)$ sono necessari $(i-1,j-1)$, $(i,j-1)$ e $(i-1,j)$. Il diagramma delle dipendenze permette di visualizzare le relazioni tra sottoistanze e di pianificare l'esecuzione in maniera appropriata. La struttura del diagramma rispecchia quella della struttura dati in cui verranno memorizzate le informazioni, $L\left[ 0 \div m, 0\div n \right]$.
\begin{equation*}
    \left[ 
    \begin{array}[H]{ccccc}
        0 & \cdots & & \cdots & 0 \\
        \vdots & (i-1,j-1) & & (i-1,j) & \\
        & & \nwarrow & \uparrow & \\
        \vdots & (i,j-1) & \leftarrow & (i,j) & \\
        0 &&&&\\
    \end{array}
    \right]
\end{equation*}
Tenuti in considerazione gli elementi che vengono popolati dal caso base, dal diagramma si evince che una scansione per righe visita gli elementi nell'ordine desiderato, trovando sempre i tre elementi su cui dipende l'istanza già correttamente calcolati. La scansione non è univoca, per esempio procedendo per colonne non si commettono errori.

L'algoritmo iterativo per trovare la sottosequenza comune tra due stringhe risulta
\begin{algorithm}[H]
\caption{\emph{Longest Common Subsequence}}\label{alg:lcsit}
\begin{algorithmic}[1]
    \Procedure{LCS}{$X,Y$}
        \State $m \gets |X|$
        \State $n \gets |Y|$
        \For{$i \gets 0 $ to $ m $ }
            % \Comment{Caso base prima colonna}
            \label{alg:lcsit:cbc}
            \State $L[i,0] \gets 0$
        \EndFor
        \For{$j \gets 1 $ to $ n $ }
            % \Comment{Caso base prima riga}
            \label{alg:lcsit:cbr}
            \State $L[0,j] \gets 0$
        \EndFor
        \For{$i \gets 1 $ to $ m $ }
            \For{$j \gets 1 $ to $ n $ }
                % \Comment{L'indice interno è di colonna}
                \label{alg:lcsit:colmaj}
                \If{$x_i = y_j$}
                \label{alg:lcsit:c1}
                    \State $L[i,j] \gets 1 + L[i-1,j-1]$
                    \State $B[i,j] \gets ` \nwarrow \textrm{'}$
                \Else
                    \If{$L[i-1,j] \geq L[i,j-1]$}
                    \label{alg:lcsit:c2}
                        \State $L[i,j] \gets L[i-1,j]$
                        \State $B[i,j] \gets ` \uparrow \textrm{'}$
                    \Else
                        \State $L[i,j] \gets L[i,j-1]$
                        \State $B[i,j] \gets ` \leftarrow \textrm{'}$
                    \EndIf
                \EndIf
            \EndFor
        \EndFor
        \State return $L[m,n], B$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Dove alle righe \ref{alg:lcsit:cbc} e \ref{alg:lcsit:cbr} si sta inizializzando il caso di base, e alla riga \ref{alg:lcsit:colmaj} la tabella sta venendo popolata secondo una scansione \emph{row-major}, in quanto l'indice interno è quello di colonna.

Nel calcolo della complessità dell'algoritmo, il confronto alla riga \ref{alg:lcsit:c1} è tra elementi della stringa, quindi viene considerato costoso, mentre quello alla riga \ref{alg:lcsit:c2} è tra interi, e si può trascurare. Si conta un confronto per iterazione del ciclo interno, per un totale di 
\begin{equation*}
    T_{LCS} = mn
\end{equation*}
In realtà i sottoproblemi utili al calcolo della soluzione sono ancora meno, ed utilizzando la versione memoizzata dell'algoritmo \emph{D\&C}, solo i sottoproblemi necessari vengono calcolati, al prezzo di utilizzare un algoritmo ricorsivo, con il suo \emph{overhead} per le chiamate (e.g. \textit{call stack}).

Una volta che si ha a disposizione $B$, stampare la sottosequenza comune è semplice:
\begin{algorithm}[H]
\caption{Stampa della \emph{Longest Common Subsequence}}\label{alg:lcsprint}
\begin{algorithmic}[1]
    \Procedure{PRINT\_LCS}{$B,i,j,X$}
        \If{$\left( i=0 \right) \vee \left( j=0 \right)$}
            \State return
        \EndIf
        \If {$B[i,j] = ` \nwarrow \textrm{'}$ }
            \State \Call{PRINT\_LCS}{$B,i-1,j-1,X$}
            \State print$\left( x_i \right)$ 
        \Else
            \If {$B[i,j] = ` \leftarrow \textrm{'}$ }
                \State \Call{PRINT\_LCS}{$B,i,j-1,X$}
            \Else
                \State \Call{PRINT\_LCS}{$B,i-1,j,X$}
            \EndIf
        \EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Implementazione dell'algoritmo \emph{LCS} memoizzto}
\begin{algorithm}[H]
\caption{\emph{Longest Common Subsequence}}\label{alg:lcsmem}
\begin{algorithmic}[1]
    \Procedure{INIT\_LCS}{$X,Y$}
        \State $m \gets |X|$
        \State $n \gets |Y|$
        \If{$\left( m=0 \right) \vee \left( n=0 \right)$}
            \State return $0$
        \EndIf
        \For{$i \gets 0 $ to $ m $ }
            \State $L[i,0] \gets 0$
        \EndFor
        \For{$j \gets 1 $ to $ n $ }
            \State $L[0,j] \gets 0$
        \EndFor
        \For{$i \gets 1 $ to $ m $ }
            \For{$j \gets 1 $ to $ n $ }
                \State $L[i,j] \gets -1$
            \EndFor
        \EndFor
        \State return \Call{REC\_LCS}{$X,Y,m,n$}
    \EndProcedure
    \Procedure{REC\_LCS}{$X,Y, i,j$}
        \If{$L[i,j]=-1$}
            \If{$x_i = y_j$}
                \State $L[i,j] \gets 1 + \Call{REC\_LCS}{X,Y,i-1,j-1}$
            \Else
                \If{$\Call{REC\_LCS}{X,Y,i,j-1} \geq \Call{REC\_LCS}{X,Y,i-1,j}$}
                \label{alg:lcsmem:c1}
                    \State $L[i,j] \gets L[i-1,j]$
                \Else
                    \State $L[i,j] \gets L[i,j-1]$
                \EndIf
            \EndIf
        \EndIf
        \State return $L[i,j]$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Notiamo che dopo la riga \ref{alg:lcsmem:c1} gli elementi $L[i-1,j]$ e $L[i,j-1]$ sono inizializzati, e possono essere assegnati senza problemi.

Nel caso peggiore l'algoritmo fa un confronto per ogni coppia di prefissi, quando trova il default. La complessità migliora quando le stringhe sono simili. Se una delle due è, sottostringa dell'altra, la complessità è lineare.

\subsection{autocomplete}
Una bella scatola:
\begin{equation}
    \boxed{x^2+y^2 = z^2}
\end{equation}

Numeri nei casi
\begin{numcases}{T(n)=}
    2^3 \label{escaso1} \\
    2^4 \label{escaso2} 
\end{numcases}

Sotto numeri
\begin{subnumcases}{T(n)=}
    2^3 \label{escaso3} \\
    2^4 
\end{subnumcases}

\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item qualcosa
    \item[+] qualcosa
    \item[*] qualcosa
    \item[--] qualcosa
\end{itemize}
àg
èg
ìg
òg
ùg
perché

delirio di vim se scrivi \verb|<C-k>`e| o \verb|<C-k>e`| in insert mode mette una è

% delirio doppio di vim se scrivi \verb|<C-k>da| in insert mode mette ``Hiragana letter DA'' che purtroppo non posso mostrarvi %だ
% insomma i digraph sono tanti e belli

Spazietti fra equazioni
\begin{equation*}
    A^{[0]}(x) = \sum_{j=0}^{\frac{n}{2}-1} a_{2j}x^j
    \quad \text{ e } \quad
    A^{[1]}(x) = \sum_{j=0}^{\frac{n}{2}-1} a_{2j+1}x^j
\end{equation*}

Un gustoso algoritmo
\begin{algorithm}[H]
\caption{Divide and Conquer}\label{alg:dncmock}
\begin{algorithmic}[1]
    \Procedure{D\&C}{$i$}
        \If{$|i| \leq n_0$}                             \Comment{BASE}
            \State *risolvo direttamente*
        \EndIf
        \State $<i_1, i_2, \dots, i_k> \gets A_D(i)$    \Comment{DIVIDE}
        \For{$j \gets 1 $ to $ k $ }                    \Comment{RECURSE}
            \State $s_j \gets $ \Call{D\&C}{$i_j$}
        \EndFor
        \State $s \gets A_C(<s_1, s_2, \dots, s_k>)$    \Comment{CONQUER}
        \State return $s$
    \EndProcedure
\end{algorithmic}
\end{algorithm}



