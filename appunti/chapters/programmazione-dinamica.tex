\section{Paradigma del \emph{Dynamic Programming}}

\subsection{Introduzione}

% TODO disegno albero TD BU pag 53.2
Il problema del memorylessness del paradigma \emph{Divide and Conquer} è già stato sottolineato nella sezione \ref{sss:alberochiamate}.
Per esemplificare il problema, si introduce la sequnza di Fibonacci, definita come
\begin{equation*}
    F_n = 
    \begin{cases}
        1 & n=0,1 \\
        F_{n-1} + F_{n-2} & n>1
    \end{cases}
\end{equation*}
da cui si ricava immediatamente un algoritmo ricorsivo
\begin{algorithm}[H]
\caption{Fibonacci ricorsivo}\label{alg:rfib}
\begin{algorithmic}[1]
    \Procedure{R\_FIB}{$n$}
        \If{$ n=0 $ or $n=1$}
            \State return $1$
        \EndIf
        \State return \Call{R\_FIB}{$n-1$} + \Call{R\_FIB}{$n-2$}
    \EndProcedure
\end{algorithmic}
\end{algorithm}
% TODO disegno albero chiamate R_FIB pag 53.4
\noindent
la cui equazione di ricorrenza è
\begin{equation*}
    T_{RF}(n) = 
    \begin{cases}
        0 & n=0,1 \\
        T_{RF}(n-1) + T_{RF}(n-2) + 1 & n>1
    \end{cases}
\end{equation*}
che risulta limitata inferiormente da un esponenziale, infatti per $n>1$
\begin{align*}
    T_{RF}(n) 
    &= T_{RF}(n-1) + T_{RF}(n-2) + 1 \\
    & \geq \, 2 T_{RF}(n-2) + 1 \\
    & \geq \, 2^2 T_{RF}(n-2-2) + 2 + 1 \\
    & \geq \, 2^i T_{RF}(n-2i) + \sum_{j=0}^{i-1} 2^j \\
    \intertext{che raggiunge il caso base quando $n-2i=0$ o $n-2i=1$, ossia per $i= \left\lfloor n/2 \right\rfloor $ sia nel caso di $n$ pari sia nel caso di $n$ dispari}
    & \geq \, 2^{\left\lfloor n/2 \right\rfloor} \cancel{ T_{RF}(0 \text{ o } 1)}
    + \sum_{j=0}^{\left\lfloor n/2 \right\rfloor -1} 2^j \\
    &= 2^{\left\lfloor n/2 \right\rfloor} -1 \\
    &=  \Omega \left( 2^{n/2} \right) = \sqrt{2}^{\,n}
    \intertext{ed essendo $\sqrt{2}>1$ cresce più velocemente di ogni polinomio. Il valore esatto di $T_{RF}$ è}
    T_{RF}(n) &= \Theta \left( \left( \frac{1+\sqrt{5}}{2} \right)^n \right)
\end{align*}

Il sugo della storia è che viene calcolata un numero molto elevato di volte la stessa sottoistanza. Ispirandosi alla fase \emph{Bottom-Up} del \emph{D\&C}, e sfruttando strutture dati che contengano le informazioni necessarie, si può scrivere un algoritmo iterativo che risolve il problema.
\begin{algorithm}[H]
\caption{Fibonacci iterativo}\label{alg:itfib}
\begin{algorithmic}[1]
    \Procedure{IT\_FIB}{$n$}
        \If{$ n=0 $ or $n=1$}
            \State return $1$
        \EndIf
        \State $F[0] \gets 1$
        \State $F[1] \gets 1$
        \For{$i \gets 2 $ to $ n $ } 
            \State $F[i] \gets F[i-1] + F[i-2]$
        \EndFor
        \State return $F[n]$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
dove le soluzioni intermedie sono salvate in $F$. In realtà sarebbe sufficiente memorizzare solo gli ultimi due valori della sequenza.

La complessità di questo algoritmo è lineare: $T_{IF}(n) = \Theta \left( n \right)$.

Si può quindi introdurre il paradigma del \emph{Dynamic Programming}, basandosi su queste osservazioni.

\subsection{Paradigma del \emph{Dynamic Programming}}
% \subsection{Introduzione}
I due concetti fondamentali su cui si basa il paradigma \emph{Dynamic Programming} sono
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item[--] dotare di memoria l'algoritmo
    \item[--] implementare la computazione in direzione \emph{bottom-up} (tutti i dati necessari sono già stati calcolati nelle iterazioni precedenti)
\end{itemize}
In ogni caso si lavora con la proprietà di sottostruttura, generando la soluzione ad un'istanza in funzione di sottoistanze di taglia minore.

Il vantaggi sono una maggiore velocità dovuta al non replicare la computazione, gli svantaggi sono legati al dover implementare la computazione in maniera da essere sicuri di avere a disposizione le soluzioni necessarie al momento giusto, che per casi articolati non è triviale. Si può sfruttare la convenienza della fase \emph{top-down}, che genera e risolve le istanze in ordine coerente, scrivendo l'algoritmo in maniera ricorsiva e dotandolo di memoria, come descritto nella sezione seguente.

\section{Memoizzazione di un algoritmo ricorsivo}

È possibile modificare un algoritmo ricorsivo \emph{D\&C} memorizzando le soluzioni intermedie attraverso un processo detto di memoizzazione.

\subsection{Metodo generale}

Un algoritmo memoizzato è costituito da due subroutine:
\begin{enumerate}
    \item \textbf{Routine di inizializzazione} INIT\_\{NomeAlg\}
        \begin{itemize}
            \item risolve i casi di base direttamente
            \item inizializza una struttura tabellare \emph{globale} con
                \begin{itemize}
                    \item valori delle istanze di base, nelle locazioni associate alle istanze di base
                    \item valori di default in posizioni associate a istanze non di base (il valore di default deve essere scelto in modo da far capire che non è stato ancora calcolato)
                \end{itemize}
            \item invoca la seconda procedura (ricorsiva) \\ % FORMATTA
        \end{itemize}
    \item \textbf{Routine ricorsiva} REC\_\{NomeAlg\}($i$)
        \begin{itemize}
            \item controlla sulla tabella per vedere se l'istanza $i$ è già stata risolta
                \begin{itemize}
                    \item se sì la ritorna
                    \item se no
                        \begin{itemize}
                            \item la calcola con la proprietà di sottostruttura
                            \item la memorizza nella tabella
                            \item la ritorna
                        \end{itemize}
                \end{itemize}
        \end{itemize}
\end{enumerate}

\textbf{Nota:} lo spazio delle sottoistanze deve essere 
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item[--] piccolo
    \item[--] facilmente indicizzabile
\end{itemize}

\subsection{Algoritmo di Fibonacci memoizzato}

Per l'algoritmo di Fibonacci le due funzioni risultano
\begin{algorithm}[H]
\caption{Fibonacci memoizzato}\label{alg:fibmemoizzato}
\begin{algorithmic}[1]
    \Procedure{INIT\_FIB}{$n$}
        \If{$ n=0 $ or $n=1$}
            \State return $1$
        \EndIf
        \State $F[0] \gets 1$, $F[1] \gets 1$
        % \State $F[1] \gets 1$
        \For{$i \gets 2 $ to $ n $ } 
            \State $F[i] \gets 0$
        \EndFor
        \State return \Call{REC\_FIB}{$n$}
    \EndProcedure
    \Procedure{REC\_FIB}{$n$}
        \If{$F[i] = 0$}
            \State $F[i] \gets \Call{REC\_FIB}{i-1} + \Call{REC\_FIB}{i-2} $
        \EndIf
        \State return $F[i]$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\textbf{Note:}
$n$ è un parametro attuale, il valore che si vuole calcolare, mentre $i$ è un parametro formale, che descrive la generica sottoistanza su cui lavora l'algoritmo ricorsivo.
Nella funzione ricorsiva si testa se la struttura tabellare contiene già la soluzione della sottoistanza, e se no lo si calcola con l'algoritmo ricorsivo e lo si salva. Nelle successive chiamate il test fallisce e non è necessario calcolare nuovamente il valore.
Il \emph{caveat} nel dare memoria a un algoritmo \emph{D\&C} è che in lo spazio delle sottoistanze deve essere molto piccolo, in questo caso una sola sottoistanza per ogni valore di $n$. Inoltre perché l'algoritmo sia efficiente lo spazio delle possibili sottoistanze da risolvere deve essere facilmente indirizzabile e salvabile in una struttura dati semplice.

\textbf{Analisi della complessità:}
La ricorrenza non riesce a catturare il fatto che l'albero delle chiamate venga tagliato ogniqualvolta il valore di una sottoistanza sia stato calcolato precedentemente.
% TODO albero delle chiamate memoizzato pag 55.9
Per la fase di inizializzazione, il numero di operazioni aritmetiche che vengono eseguite è nullo.
Dall'albero delle chiamate della procedura ricorsiva con memoria, si nota che l'albero è diventato molto più snello, e sono comparse foglie dove prima erano presenti sottoalberi. Inoltre il numero di nodi interni, gli unici rispetto a cui viene fatto lavoro, sono pari al numero di sottoistanze \emph{distinte} che vanno risolte per ottenere l'istanza $n$. 
Solo nel nodo interno si esegue il conquer (la somma) associato ad ogni chiamata e la complessità sarà corrispondente al numero di nodi interni per il lavoro compiuto in ciascun nodo. C'è un nodo interno per ogni chiamata \emph{non} di base, tutte le altre sono chiamate che hanno trovato il valore in $F$ e non hanno compiuto lavoro, se non un \emph{lookup} nella tabella.
\begin{equation*}
    T_{RF} (n) = n-2+1 = n-1
\end{equation*}

\section{Paradigma generale \emph{Dynamic Programming}}

\subsection{Problemi di ottimizzazione combinatoria}

Ricordiamo la definizione di problema computazionale $\bpi{} \subseteq \bi{} \times \bs{}$

Si può definire per ogni istanza un sottoinsieme $\bs{}(i)$ formato da tutte le possibili soluzioni di quell'istanza, ricordando che una sottoistanza può ammetere più soluzioni.
\begin{equation*}
    \forall i \in \bs{} \quad S(i) = \left\{ s \in \bs{} : i \, \bpi{} \, s \right\}
\end{equation*}

Definiamo inoltre una funzione di costo che va da S in un qualche insieme totalmente ordinato.
\begin{equation*}
    c : \bs{} \to \mathbb{R}
\end{equation*}

Data $i$ un istanza generica, si vuole determinare non solo una soluzione $s \in \bs{}(i)$, ma individuare $s^*$ che massimizza (o minimizza) il criterio del costo.
\begin{equation*}
    c(s^*) = \max \left\{ c(s) : s \in \bs{}(i) \right\}
\end{equation*}

\subsection{Caratteristiche di problemi di ottimizzazione risolubili con la programmazione dinamica}

Un problema si presta ad essere risolto con la programamzione dinamica se presenta la seguente proprietà di sottostruttura ottima, che per un prolbema di ottimizzazione è una proprietà molto più ristretta rispetto a quella necessaria per il \emph{D\&C}. Infatti mette in relazione soluzioni ottime di un istanza a soluzioni ottime di sottoistanze. Questa proprietà si dice anche \emph{optimal substructure property}.

% Dare paradigmi generali che si applicano quando un problema va approcciato con la programamzione dinamica.

% Come nel dnc si sviluppa una proprietà di sottostruttura, possiamo seguire una serie di indicazioni generali che si possono istanziare volta per volta che conducono in maniera ordinata alla risuluzone di un problema con la prog dinanica. 

% Quando si decide di usare la prog dinamica?

\begin{definition}[Proprietà di sottostruttura ottima]
    Un problema gode della proprietà di sottostruttura ottima se
    \begin{enumerate}
        \item la soluzione ottima di un'istanza non di base si ottiene combinando soluzioni ottime di sottoistanze
        \item la proprietà di sottostruttura ottima genera sottoproblemi ripetuti
        \item lo spazio delle sottoistanze generate da una data istanza non è troppo grande
    \end{enumerate}
\end{definition}

Se la proprietà 2 manca, si può applicare il \emph{D\&C} classico e la programamzione dinamica non aiuterebbe.
La proprietà 3 assicura l'efficienza del paradigma, il concetto è arbitrario per ora, se non esiste una struttura dati che riesca a memorizzare in maniera efficiente i risultati intermedi, il metodo avrà risultati peggiori. In genere la dimensionalità della struttura è pari al numero di sottoistanze generate (e.g. se istanze di taglia $|i|=n$ generano $n^2$ sottoistanze, possono essere salvate in un array \emph{bi}dimensionale, $n^3$ \emph{tri}dimensionale e così via).

\subsection{Paradigma generale}

Presentiamo un paradigma generale per la programmazione dinamica, enumerando i passi da seguire per sviluppare un programma secondo il \emph{dynamic programming}, garantendo implicitamente la correttezza del codice.

\begin{enumerate}
    \item caratterizza la struttura di una soluzione ottima $s^*$ a un'istanza $i$ non di base in funzione di soluzioni ottime $s_1^*, s_2^*, \cdots, s_k^*$ di sottoistanze di $i$
    \item
        \begin{enumerate}
            \item determina una relazione di ricorrenza sui costi di istanza e sottoistanza, mettendo in relazione il costo di un'istanza come una funzione dei costi delle sottoistanze \\ $c(s^*)=f\left(c(s_1^*),c(s_2^*),\cdots,c(s_k^*)\right)$
                \label{enum:pd1}
            \item determina la minima informazione strutturale necessaria ad ottenere $s^*$ a partire da $s_1^*, s_2^*, \cdots, s_k^*$, cercando di memorizzare incrementi, e non le intere soluzioni di sottoistanze
                \label{enum:pd2}
        \end{enumerate}
    \item
        \begin{enumerate}
            \item calcola il costo $c(s^*)$ utilizzando la ricorrenza impostando la computazione
                \begin{itemize}
                    \item in maniera \emph{bottom-up} iterativa
                    \item in maniera memoizzata
                \end{itemize}
                \label{enum:pd3}
            \item calcola l'informazione addizionale strutturale per ottenere $s^*$ e memorizzarla
                \label{enum:pd4}
        \end{enumerate}
\end{enumerate}

I punti \ref{enum:pd1}, \ref{enum:pd3} sono sufficienti per ricavare il costo della soluzione ottima,
mentre i punti \ref{enum:pd2}, \ref{enum:pd4} sono necessari quando si è interessati anche al valore della soluzione ottima.

\section{Ricerca della \emph{Longest Common Subsequence}}

\subsection{Sottostringhe e sottosequenze}

\subsubsection{Notazione}

\begin{itemize}
    \item[--] $\Sigma$ un alfabeto finito di simboli
    \item[--] $\Sigma^{*}$ la sua stella di \emph{Kleene}, l'insieme infinito di concatenazioni finite di simboli in $\Sigma$
    \item[--] $X = < x_1, x_2, \cdots, x_m > \in \Sigma^{*}$ una stringa di lunghezza $m = |X|$
    \item[--] $X = \varepsilon$ la stringa vuota
\end{itemize}

\subsubsection{Prefisso, suffisso, sottostringa}

\begin{definition}[Prefisso]
    Data una stringa $X$ con $|X|=m$ 
    un prefisso proprio
    è formato dai primi $i$ simboli di $X$.
    \[
        {X_i = < x_1, \cdots, x_i >}
        \quad \text{con} \quad
        1 \leq i \leq m
    \]
    Il prefisso improprio $X_0$ è definito come la stringa vuota.
    \label{def:prefisso}
\end{definition}

\begin{definition}[Suffisso]
    Data una stringa $X$ con $|X|=m$ 
    un suffisso proprio
    è formato dagli ultimi $j$ simboli di $X$.
    \[
        {X^j = < x_j, \cdots, x_m >}
        \quad \text{con} \quad
        1 \leq j \leq m
    \]
    Il prefisso improprio $X^{m+j}$ è definito come la stringa vuota.
    \label{def:suffisso}
\end{definition}

\begin{definition}[Sottostringa]
    Data una stringa $X$ con $|X|=m$ 
    una sottostringa
    è formata da $j-i+1$ simboli contigui di $X$.
    \[
        X_{i..j} = < x_i, \cdots, x_j >
        \quad \text{con} \quad
        1 \leq i \leq j \leq m
    \]
    Se $i>j$, $X_{i..j}=\varepsilon$
    \label{def:sottostringa}
\end{definition}

Lo spazio delle sottoistanze di una stringa $|X|=m$ risulta
\begin{align*}
    1 + \sum_{i=1}^{m} \sum_{j=i+1}^{m} 1 
    &= 
    1 + \sum_{i=1}^{m} \left( m-i+1 \right)
    &
    i'=m-i+1
    \\
    &=
    1 + \sum_{i'=1}^{m} i' 
    \\
    &=
    1 + \frac{m\left( m+1 \right)}{2}
    \\
    &=
    \Theta \left( m^2 \right)
\end{align*}
dove avendo già considerata una volta la sottostringa impropria, l'indice $i$ può variare a piacimento, mentre l'indice $j$ ne è sempre maggiore.

\subsubsection{Sottosequenza}

\begin{definition}[Sottosequenza]
    Data una stringa $X$ con $|X|=m$,
    $Z = < z_1, \cdots, z_k >$ è una sottosequenza di $X$ se esiste una successione di indici crescente
    % $
    \[
    1 \leq i_1 < i_2 < \cdots < i_k \leq m
    \]
    % $
    tale che 
    $z_j = x_{i_j}$, $1 \leq j \leq k$
    % \\
    , ed è la successione di indici crescente che \emph{realizza} $Z$ in $X$
    \label{def:sottosequenza}
\end{definition}

Per esempio, sia $X = <A,B,C,B,B,D>$, una sua sottostringa è $Z_1 = X_{1..3} = <A,B,C>$ mentre una sua sottosequenza è $Z_2 = <A,C,B>$. La stessa sottosequenza può essere realizzata da più successioni di indici, infatti: $i_1=1, \, i_2=3, \, i_3=\left\{ 4,5 \right\}$

Una sottostringa è sempre anche una sottosequenza, infatti $X_{k..s}$ è realizzata da $i_1=k, \, i_2=k+1, \, \cdots, \, i_{s-k+1}=s$

Lo spazio delle sottoistanze è considerevolmente più ampio, infatti il numero di possibili sottoinsiemi ordinati di indici $S \subseteq \left\{ 1, \cdots, m \right\}$, che quindi generano una sottosequenza valida, è pari a $2^m$.

\subsection{Definizione del problema}

Si può quindi introdurre il problema della ricerca della sottosequenza comune \emph{più lunga} tra due stringhe. Formalmente:

\begin{definition}
    Data un'istanza $(X,Y) \in \Sigma^* \times \Sigma^*$, con
    $ X = < x_1, \cdots, x_m > $ e 
    $ Y = < y_1, \cdots, y_n > $,
    allora
    $ Z = < z_1, \cdots, z_k > $
    è sottosequenza comune di $X$ e $Y$ se $Z$ è sottosequenza di entrambe le stringhe.
    L'obiettivo è determinare la sottosequenza comune di massima lunghezza 
    $Z^* = LCS\left( X,Y \right)$
    \label{def:lcs}
\end{definition}
Per provare che una sottosequenza sia comune è sufficiente esibire le due successioni di indici che realizzano $Z$ in $X$ e $Y$, che sia la sottosequenza di lunghezza massima viene assicurato dall'algoritmo che risolve il problema.

Spesso invece di voler individuare $Z^*$ è sufficiente conoscerne il costo $|Z^*|$, di cui si sa che $|LCS(X,Y)| < \min\left\{ m,n \right\}$.

Si può stabilire se $Z$ è sottosequenza di $X$ in tempo $\Theta \left( |X| \right)$, da cui si ricava un algoritmo naive per la ricerca della $LCS$, generando tutte le possibili sottosequenze della stringa più corta e verificando siano sottosequenze della più lunga. Se $m \leq n$, la complessità risulta $\Theta \left( 2^{m} n \right)$.

\subsection{Proprietà di sottostruttura ottima}

Supponiamo di star analizzando la sottoistanza di taglia $m+n$ relativa alle stringhe
% $ X = < x_1, \cdots, x_m > $, $ Y = < y_1, \cdots, y_n > $, e sia
\[
    X = < x_1, \cdots, x_m > \quad  Y = < y_1, \cdots, y_n >
\]
 e sia
$ Z^* = < z_1, \cdots, z_k > $
la soluzione ottima associata a questa sottoistanza. 
Soffermiamoci sull'ultimo elemento delle due stringhe:
% $ X = < X_{m-1}, a > $ e $ Y = < Y_{n-1}, b > $.
\[
     X = < X_{m-1}, a >  \quad  Y = < Y_{n-1}, b >
\]
Si possono presentare due casi.
\begin{enumerate}
    \item $a=b$: di sicuro $z_k=a=b$ e $ Z^* = < Z_{k-1}^*, a > $: intuitivamente se $Z^*$ non terminasse con $a$, esisterebbe $\widehat{Z}$ di lunghezza massima che termina con $\widehat{z}_k \neq a$ ed è sottosequenza di $X_{m-1}^*$ e $Y_{n-1}^*$ (non terminando con $a$), ma appendendo $a$ a $\widehat{Z}$ si otterrebbe una sottosequenza più lunga, che è assurdo.
        \\
        Inoltre $Z_{k-1}^*$ sta in $X_{m-1}^*$ e $Y_{n-1}^*$, avendo già considerato l'ultimo elemento di entrambe.
        \\
        La sottoistanza generata ha taglia $m+n-2$
    \item $a \neq b$: di sicuro $z_k$ non può essere uguale ad $a$ e $b$ contemporaneamente (e può comunque essere diverso da entrambi i valori). Deve quindi verificarsi uno dei due sottocasi:
        \begin{enumerate}[label=(\roman*)]
            \item $z_k \neq a$: va risolto $LCS\left( X_{m-1}, Y \right)$
            \item $z_k \neq b$: va risolto $LCS\left( X, Y_{n-1} \right)$
        \end{enumerate}
        In entrambi i casi la sottoistanza generata ha taglia $m+n-1$
\end{enumerate}

Lo spazio dei sottoproblemi è $\left\{ \left( X_i, Y_j \right), \: 0 \leq i \leq m, \: 0 \leq j \leq n \right\}$ ed ha quindi taglia pari a $(m+1)(n+1) = \Theta (mn)$

Formalizzando la proprietà di sottostruttura ottima risulta:

\begin{lemma}[Proprietà di sottostruttura ottima per la \emph{Longest Common Subsequence}]
    Per un generico sottoproblema $\left( X_i, Y_j \right), \: 0 \leq i \leq m, \: 0 \leq j \leq n$ sia 
    $ Z^* = < z_1, \cdots, z_k > = LCS\left( X_i,Y_j \right)$ allora:
    \begin{enumerate}
        \item $(i=0) \vee (j=0) \Rightarrow z^* = \varepsilon$
            \label{psolcs:casobase}
        \item $(i>0) \wedge (j>0) \wedge (x_i = y_j) \Rightarrow $
            \begin{enumerate}
                \item $z_k = x_i = y_j$
                    \label{psolcs:caso2a}
                \item $Z_{k-1}^* = LCS(X_{i-1}, Y_{j-1})$
                    \label{psolcs:caso2b}
            \end{enumerate}
        \item $(i>0) \wedge (j>0) \wedge (x_i \neq y_j) \Rightarrow $
            \\
            $ Z^*$ è la stringa più lunga tra $LCS\left( X_{i-1}, Y_j \right)$ e $LCS\left( X_i, Y_{j-1} \right)$
            \label{psolcs:caso3}
    \end{enumerate}
    \label{lemma:psolcs}
\end{lemma}

\begin{proof}[Caso \ref{psolcs:casobase}]
    La dimostrazione del caso \ref{psolcs:casobase} è immediata, infatti l'unica sottosequenza comune è la stringa vuota $\varepsilon$.
    \[
        LCS(\varepsilon, Y_j), LCS(X_i, \varepsilon) \Rightarrow Z^*=\varepsilon 
    \]
\end{proof}

\begin{proof}[Caso \ref{psolcs:caso2a}]
    La dimostrazione del caso \ref{psolcs:caso2a} si svolge per assurdo:
    Sia 
    $ Z^* = < z_1, \cdots, z_k > $
    la soluzione ottima realizzata da 
    $1 \leq i_1 < i_2 < \cdots < i_k \leq i $ in $X_i$
    e da
    $1 \leq j_1 < j_2 < \cdots < j_k \leq j $ in $Y_j$
    .\\
    Si supponga per assurdo che
    \[
        z_k \neq x_i ( \neq y_j)
    \]
    Per $1 \leq s \leq k$ il carattere generico $z_s$ è
    \[
        z_s = x_{i_s} = y_{j_s}
    \]
    Applicando l'ipotesi assurda rispetto a  $z_s$ per $s=k$ vale
    \[
        z_k = x_{i_k} \neq x_i \quad z_k = y_{j_k} \neq y_j
    \]
    devono quindi essere diversi gli indici
    \[
        (i_k \neq i) \; \wedge \; (j_k \neq j)
    \]
    e nelle successioni di indici allora
    \[
        (i_k<i \text{ o } i_k \leq i-1) 
        \; \wedge \;
        (j_k<j \text{ o } j_k \leq j-1)
    \]
    per cui $Z^*$ è sottosequenza comune di $X_{i-1}$ e $Y_{j=i}$.
    Si consideri $\widehat{Z} = < Z^* , x_i >$, $\widehat{Z}$ è sottosequenza sia di $X_i$ sia di $Y_j$
    e le successioni di indici da esibire sono quelle di $Z^*$ con appesi $i_k < i_{k+1} = i$ e $j_k < j_{k+1} = j$.
    Quindi si trova una sottosequenza comune più lunga di $Z^*$, ma $Z^*$ era supposta come soluzione ottima, quindi è stato raggiunto un assurdo.
\end{proof}

\begin{proof}[Caso \ref{psolcs:caso2b}]
    Anche la dimostrazione del caso \ref{psolcs:caso2b} si svolge per assurdo:
    Assumendo che la soluzione ottima $Z^*$ sia realizzata dalle stesse successioni di indici della dimostrazione precedente, gli indici che realizzano
    $Z_{k-1}^* = LCS(X_{m-1}^*, Y_{n-1}^*)$
    sono i primi $k-1$ indici che realizzano $Z^*$.
    \[
        \begin{array}{c|cc}
            1 \leq i_1 < i_2 < \cdots < i_{k-1} & < \cancel{i_k} = i  & \text{ in } X_{i-1}
            \\
            1 \leq j_1 < j_2 < \cdots < j_{k-1} & < \cancel{j_k} = j  & \text{ in } Y_{j-1}
        \end{array}
    \]
    Quindi $i_{k-1} \leq i-1$ e $j_{k-1} \leq j-1$
    per cui 
    $Z_{k-1}^* $ è sottosequenza comune di $X_{i-1}^* $ e $ Y_{j-1}^*$ ed è dunque una soluzione ammissibile; si dimostra anche che è la soluzione ottima.
    \\
    Si supponga per assurdo che
    $Z_{k-1}^* \neq LCS(X_{i-1}^*, Y_{j-1}^*)$
    , allora deve esistere $\bar{Z}$ comune a $X_{i-1}^* $ e $ Y_{j-1}^*$ di lunghezza
    % \[
    $
    | \bar{Z} | > | Z_{k-1}^* | = k-1
    % \]
    $.
    Considerando $\widehat{Z} = < \bar{Z} , x_i > $, $\widehat{Z}$ è sottosequenza comune di $X_i$ e $Y_j$ di lunghezza
    \[
        | \widehat{Z} | = | \bar{Z} | + 1 > k -1 +1 = k = | Z^* |
    \]
    Il che conduce ad un assurdo, essendo $Z^*$ la soluzione ottima.
\end{proof}

\begin{proof}[Caso \ref{psolcs:caso3}]
    Anche la dimostrazione del caso \ref{psolcs:caso3} si svolge per assurdo:
    Sia $ Z^*$ la stringa più lunga tra $LCS\left( X_{i-1}, Y_j \right)$ e $LCS\left( X_i, Y_{j-1} \right)$,
    realizzata da 
    $1 \leq i_1 < i_2 < \cdots < i_k \leq i $ in $X_i$
    e da
    $1 \leq j_1 < j_2 < \cdots < j_k \leq j $ in $Y_j$.
    \\
    Si deve verificare uno dei due casi
    $( z_k \neq x_i ) \vee (z_k \neq y_j)$:
    \\
    Nel primo caso
    $ z_k \neq x_i \Rightarrow i_k<i$
    e $i_k \leq i-1$
    quindi $Z^*$ è in questo caso sottosequenza del prefisso più corto $X_{i-1}$ e come in precedenza è sottosequenza di $Y_j$. È quindi una sottosequenza comune a $X_{i-1}$ e $Y_j$ e deve essere $Z^* = LCS(X_{i-1}, Y_{j})$. Se per assurdo non lo fosse, esisterebbe $\widehat{Z}^* = LCS(X_{i-1}, Y_{j})$ con $|\widehat{Z}| > |Z^*|$, ma $\widehat{Z}$ è anche comune a $X_{i}$ e $Y_j$ che è un assurdo perché $Z^*$ è ottima.
    \\
    Nel secondo caso $Z^* = LCS(X_{i}, Y_{j-1})$, dimostrato analogamente.
    \\
    Dato che uno dei due casi deve succedere, $Z^*$ sarà la più lunga delle due $LCS$ trovate.
\end{proof}
% note su prop strutturali TODO pag 64
La funzione di costo è di due parametri $0 \leq i \leq m$ e $0 \leq j \leq n$
\begin{equation*}
    l(i,j) = 
    \begin{cases}
        0 & (i=0) \vee (j=0)
        \\
        1+l(i-1,j-1) & (i,j>0) \wedge (x_i = y_j)
        \\
        \max \left\{ l(i-1,j) , l(i,j-1) \right\}
        & \text{altrimenti}
    \end{cases}
\end{equation*}
% TODO esempio albero pag 64.5
Il caso peggiore è caratterizzato da $m=n$ quando tutti i caratteri sono diveri tra loro e $LCS=\varepsilon$ e l'albero si biforca sempre.
\begin{equation*}
    T(n,n) = 
    \begin{cases}
        0 & n=0 \\
        T(n-1, n) + T(n, n-1) + 1 & n>0
    \end{cases}
\end{equation*}
Essendo una funzione monotona
\begin{align*}
    T(n,n) &\geq 2T(n-1,n-1)+1
    \intertext{e si può procedere per unfolding:}
    & \geq 2 \cdot 2 T(n-1-1,n-1-1)+2+1
    \\
    \dots & \geq 2^j T(n-j,n-j)+\sum_{k=0}^{j-1}2^k
    \intertext{il caso base si raggiugne per $n-j=0$ ossia $j=n$}
    & \geq \sum_{j=0}^{n-1}2^k = 2^{n-1}
\end{align*}
Seguendo il paradigma \emph{D\&C} anche solo calcolare il costo della souzione ottima è esponenziale.

\subsection{Implementazione dell'algoritmo \emph{LCS} iterativo}
Per applicare il paradigma del \emph{Dynamic Programming}, occorre individuare la minima informazione addizionale per ricostruire la soluzione ottima.
In un vettore bidimensionale si può salvare quale caso è successo, memorizzando uno di tre simboli:
\begin{equation*}
    b(i,j) = 
    \begin{cases}
        ` \nwarrow \textrm{'}
        &
        LCS(X_i, Y_j)
        = < LCS(X_{i-1}, Y_{j-1}), x_i >
        \\
        ` \uparrow \textrm{'}
        &
        LCS(X_i, Y_j)
        = LCS(X_{i}, Y_{j-1})
        \\
        ` \leftarrow \textrm{'} 
        &
        LCS(X_i, Y_j)
        = LCS(X_{i-1}, Y_{j})
    \end{cases}
\end{equation*}

Si può seguire un algoritmo iterativo per calcolare il costo, avendo cura di aver già calcolato i sottoproblemi necessari al momento giusto. Per l'istanza $(i,j)$ sono necessari $(i-1,j-1)$, $(i,j-1)$ e $(i-1,j)$. Il diagramma delle dipendenze permette di visualizzare le relazioni tra sottoistanze e di pianificare l'esecuzione in maniera appropriata. La struttura del diagramma rispecchia quella della struttura dati in cui verranno memorizzate le informazioni, $L\left[ 0 \div m, 0\div n \right]$.
\begin{equation*}
    \left[ 
    \begin{array}[H]{ccccc}
        0 & \cdots & & \cdots & 0 \\
        \vdots & (i-1,j-1) & & (i-1,j) & \\
        & & \nwarrow & \uparrow & \\
        \vdots & (i,j-1) & \leftarrow & (i,j) & \\
        0 &&&&\\
    \end{array}
    \right]
\end{equation*}
Tenuti in considerazione gli elementi che vengono popolati dal caso base, dal diagramma si evince che una scansione per righe visita gli elementi nell'ordine desiderato, trovando sempre i tre elementi su cui dipende l'istanza già correttamente calcolati. La scansione non è univoca, per esempio procedendo per colonne non si commettono errori.

L'algoritmo iterativo per trovare la sottosequenza comune tra due stringhe risulta
\begin{algorithm}[H]
\caption{\emph{Longest Common Subsequence}}\label{alg:lcsit}
\begin{algorithmic}[1]
    \Procedure{LCS}{$X,Y$}
        \State $m \gets |X|$
        \State $n \gets |Y|$
        \For{$i \gets 0 $ to $ m $ }
            % \Comment{Caso base prima colonna}
            \label{alg:lcsit:cbc}
            \State $L[i,0] \gets 0$
        \EndFor
        \For{$j \gets 1 $ to $ n $ }
            % \Comment{Caso base prima riga}
            \label{alg:lcsit:cbr}
            \State $L[0,j] \gets 0$
        \EndFor
        \For{$i \gets 1 $ to $ m $ }
            \For{$j \gets 1 $ to $ n $ }
                % \Comment{L'indice interno è di colonna}
                \label{alg:lcsit:colmaj}
                \If{$x_i = y_j$}
                \label{alg:lcsit:c1}
                    \State $L[i,j] \gets 1 + L[i-1,j-1]$
                    \State $B[i,j] \gets ` \nwarrow \textrm{'}$
                \Else
                    \If{$L[i-1,j] \geq L[i,j-1]$}
                    \label{alg:lcsit:c2}
                        \State $L[i,j] \gets L[i-1,j]$
                        \State $B[i,j] \gets ` \uparrow \textrm{'}$
                    \Else
                        \State $L[i,j] \gets L[i,j-1]$
                        \State $B[i,j] \gets ` \leftarrow \textrm{'}$
                    \EndIf
                \EndIf
            \EndFor
        \EndFor
        \State return $L[m,n], B$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Dove alle righe \ref{alg:lcsit:cbc} e \ref{alg:lcsit:cbr} si sta inizializzando il caso di base, e alla riga \ref{alg:lcsit:colmaj} la tabella sta venendo popolata secondo una scansione \emph{row-major}, in quanto l'indice interno è quello di colonna.

Nel calcolo della complessità dell'algoritmo, il confronto alla riga \ref{alg:lcsit:c1} è tra elementi della stringa, quindi viene considerato costoso, mentre quello alla riga \ref{alg:lcsit:c2} è tra interi, e si può trascurare. Si conta un confronto per iterazione del ciclo interno, per un totale di 
\begin{equation*}
    T_{LCS} = mn
\end{equation*}
In realtà i sottoproblemi utili al calcolo della soluzione sono ancora meno, ed utilizzando la versione memoizzata dell'algoritmo \emph{D\&C}, solo i sottoproblemi necessari vengono calcolati, al prezzo di utilizzare un algoritmo ricorsivo, con il suo \emph{overhead} per le chiamate (e.g. \textit{call stack}).

Una volta che si ha a disposizione $B$, stampare la sottosequenza comune è semplice:
\begin{algorithm}[H]
\caption{Stampa della \emph{Longest Common Subsequence}}\label{alg:lcsprint}
\begin{algorithmic}[1]
    \Procedure{PRINT\_LCS}{$B,i,j,X$}
        \If{$\left( i=0 \right) \vee \left( j=0 \right)$}
            \State return
        \EndIf
        \If {$B[i,j] = ` \nwarrow \textrm{'}$ }
            \State \Call{PRINT\_LCS}{$B,i-1,j-1,X$}
            \State print$\left( x_i \right)$ 
        \Else
            \If {$B[i,j] = ` \leftarrow \textrm{'}$ }
                \State \Call{PRINT\_LCS}{$B,i,j-1,X$}
            \Else
                \State \Call{PRINT\_LCS}{$B,i-1,j,X$}
            \EndIf
        \EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Implementazione dell'algoritmo \emph{LCS} memoizzto}
\begin{algorithm}[H]
\caption{\emph{Longest Common Subsequence}}\label{alg:lcsmem}
\begin{algorithmic}[1]
    \Procedure{INIT\_LCS}{$X,Y$}
        \State $m \gets |X|$
        \State $n \gets |Y|$
        \If{$\left( m=0 \right) \vee \left( n=0 \right)$}
            \State return $0$
        \EndIf
        \For{$i \gets 0 $ to $ m $ }
            \State $L[i,0] \gets 0$
        \EndFor
        \For{$j \gets 1 $ to $ n $ }
            \State $L[0,j] \gets 0$
        \EndFor
        \For{$i \gets 1 $ to $ m $ }
            \For{$j \gets 1 $ to $ n $ }
                \State $L[i,j] \gets -1$
            \EndFor
        \EndFor
        \State return \Call{REC\_LCS}{$X,Y,m,n$}
    \EndProcedure
    \Procedure{REC\_LCS}{$X,Y, i,j$}
        \If{$L[i,j]=-1$}
            \If{$x_i = y_j$}
                \State $L[i,j] \gets 1 + \Call{REC\_LCS}{X,Y,i-1,j-1}$
            \Else
                \If{$\Call{REC\_LCS}{X,Y,i,j-1} \geq \Call{REC\_LCS}{X,Y,i-1,j}$}
                \label{alg:lcsmem:c1}
                    \State $L[i,j] \gets L[i-1,j]$
                \Else
                    \State $L[i,j] \gets L[i,j-1]$
                \EndIf
            \EndIf
        \EndIf
        \State return $L[i,j]$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Notiamo che dopo la riga \ref{alg:lcsmem:c1} gli elementi $L[i-1,j]$ e $L[i,j-1]$ sono inizializzati, e possono essere assegnati senza problemi.

Nel caso peggiore l'algoritmo fa un confronto per ogni coppia di prefissi, quando trova il default. La complessità migliora quando le stringhe sono simili. Se una delle due è sottostringa dell'altra, la complessità è lineare.


\section{\emph{Matrix Chain Multiplication}}

\subsection{Definizione del problema}

La moltiplicazione tra matrici è definita anche fra matrici rettangolari, a patto che il numero di colonne della matrice sinistra sia pari al numero di righe della matrice destra.
\begin{equation*}
\begingroup
\setlength{\arraycolsep}{2pt} % horizontal
\renewcommand{\arraystretch}{0.5} % vertical spacing
    \begin{array}[h]{ccccccccccc}
          & A &   & \times &   & B &   & = &   & C & \\%[-4pt]
        m &   & n &        & n &   & p &   & m &   & p
    \end{array}
\endgroup
\end{equation*}

L'algoritmo per moltiplicare matrici quadrate viene modificato lievemente:
\begin{algorithm}[H]
\caption{Prodotto di matrici rettangolari}\label{alg:mulret}
\begin{algorithmic}[1]
\Procedure{RECT\_MUL}{$A,B$}
    \State $m \gets A.rows$
    \State $n \gets A.cols$
    \State $p \gets B.rows$
    \For{$i \gets 1 $ to $ m $ }
        \For{$j \gets 1 $ to $ p $ }
            \State $c_{i,j} \gets 0$
            \For{$k \gets 1 $ to $ n $ }
                \State $c_{i,j} \gets c_{i,j} + a_{i,k} \cdot b_{k,j} $
            \EndFor
        \EndFor
    \EndFor
    \State return $C$
\EndProcedure
\end{algorithmic}
\end{algorithm}
La complessità risulta una funzione di tre parametri
\begin{equation*}
    T\left( m,n,p \right) = mnp
\end{equation*}

La moltiplicazione si generalizza a catene di matrici, sempre mantendo la compatibilità tra le dimensioni
\begin{equation*}
\begingroup
\setlength{\arraycolsep}{2pt} % horizontal
\renewcommand{\arraystretch}{0.5} % vertical spacing
    \begin{array}[h]{ccccccccccccc}
            & A_1 &     & A_2 &     & \cdots &         & A_i &     & \cdots &         & A_n &   \\%[-4pt]
        p_0 &     & p_1 &     & p_2 &        & p_{i-1} &     & p_i &        & p_{n-1} &     & p_n
    \end{array}
\endgroup
\end{equation*}
La catena di moltiplicazioni può quindi essere rappresentata da un array di $n+1$ dimensioni (massime) distinte
\begin{equation*}
    \vec{p} \in \left( \mathbb{N}^+ \right)^{n+1}
\end{equation*}
La moltiplicazione \emph{non} è commutativa, ma è associativa. Questo comporta una certa libertà nell'ordine con cui si effettuano le moltiplicazioni, che influisce molto sul numero totale di prodotti tra elementi da eseguire. Per esempio
\begin{equation*}
\begingroup
\setlength{\arraycolsep}{2pt} % horizontal
\renewcommand{\arraystretch}{0.5} % vertical spacing
    \begin{array}[h]{ccccccc}
           & A_1 &     & A_2 &   & A_3 &   \\  
        10 &     & 100 &     & 5 &     & 50
    \end{array}
\endgroup
\end{equation*}
può essere svolta nei seguenti modi 
\begin{equation*}
\left( \left( A_1 A_2 \right) A_3 \right)
\quad
\text{o}
\quad
\left( A_1 \left( A_2 A_3 \right) \right)
\end{equation*}
che risultano nei seguenti passi intermedi
\begin{equation*}
\begingroup
\setlength{\arraycolsep}{2pt} % horizontal
\renewcommand{\arraystretch}{0.5} % vertical spacing
    \begin{array}[h]{ccccccc}
           & A_{1,2} &   & A_3 &   \\  
        10 &         & 5 &     & 50
    \end{array}
\quad
\text{o}
\quad
    \begin{array}[h]{ccccccc}
           & A_{1} &     & A_{2,3} &   \\  
        10 &       & 100 &         & 50
    \end{array}
\endgroup
\end{equation*}
Nel primo caso però il numero di operazioni richieste è $10 * 100 * 5 + 10 * 5 * 50 = 7500$
mentre nel secondo è $100 * 5 * 50 + 10 * 100 * 50 = 75000$. Una buona parentesizzazione permette quindi di risparmiare un gran numero di operazioni.

\subsection{Proprietà di sottostruttura ottima}

Ad ogni parentesizzazione è associato un albero binario pieno, con $n$ foglie e $n-1$ nodi interni. Il numero di parentesizzazioni è quindi pari al numero di possibili alberi, ossia $\Theta \left( 4^n / n^{3/2} \right)$.
Risolvere il problema per enumerazione è chiaramente impraticabile.

TODO albero parentesizzazione esempio, pag 70

Il problema della \emph{Matrix Chain Multiplication} può essere quindi riscritto come:
\\
\textbf{Ingresso:} $ \vec{p} \in \left( \mathbb{N}^+ \right)^{n+1} $
\\
\textbf{Uscita:} $T^*$ parentesizzazione ottima

TODO albero parentesizzazione binaria, pag 70.5

Si nota che la parentesizzazione $T^*$ è composta da un nodo radice (che rappresenta una moltiplicazione) e due sottoalberi, anch'essi alberi binari pieni, che sono parentesizzazioni di catene più corte di matrici contigue. Si può quindi intuire la presenza di una proprietà di sottostruttura, di cui si dimostra la correttezza e l'ottimalità.

Si supponga di stare analizzando il sottoproblema generico $A_{i..j}$, con associata la parentesizzazione $T_{i..j}^{*}$, ossia la moltiplicazione ottima di $A_i, A_{i+1},\ldots, A_j$ per $1 \leq i \leq j \leq n$.
\\
Nel caso base $i=j$ la catena ha una sola matrice, $A_{i..j} = A_i$ e l'albero è composto dal solo nodo radice.
\\
Per il caso non di base $i<j$, si supponga di conoscere la parentesizzazione ottima
$T_{i..j}^*$
e ne si analizzi la struttura. L'albero è composto dal nodo radice e da due sottoalberi, 
$T_{i.. \bar{k}}$
e
$T_{\bar{k}+1..j}$
, con $i \leq \bar{k} < j$. Va dimostrata l'ottimalità delle sottoistanze, ossia
$T_{i.. \bar{k}} = T_{i.. \bar{k}}^*$ e 
$T_{\bar{k}+1..j} = T_{\bar{k}+1..j}^*$.
Mettendo in relazione il costo di $T_{i..j}^*$ con i costi di $T_{i.. \bar{k}}$ e $T_{\bar{k}+1..j}$, si ottiene:
\begin{equation*}
    c \left( 
T_{i..j}^*
    \right)
    =
    c \left( 
T_{i.. \bar{k}}
    \right)
    +
    c \left( 
T_{\bar{k}+1..j}
    \right)
    + p_{i-1} \: p_{\bar{k}} \: p_j
\end{equation*}
L'ottimalità dei sottoproblemi si dimostra per \emph{cut and paste}: se per assurdo $ T_{i.. \bar{k}} $ non fosse la parentesizzazione ottima di $A_{i..\bar{k}}$, si potrebbe sostituire il sottoalbero con quello ottimo. Ma in questo modo il costo di $ T_{i..j}^* $ diminuirebbe, che è un assurdo perché è la soluzione ottima di $A_{i..j}$.
\\
Il valore di $\bar{k}$ che minimizza il costo si ricava come
\begin{equation*}
    \bar{k}
    =
    \argmin_{1 \leq \bar{k} < j}
    \left\{ 
    c \left( 
    T_{i.. \bar{k}}^*
    \right)
    +
    c \left( 
    T_{\bar{k}+1..j}^*
    \right)
    + p_{i-1} \: p_{\bar{k}} \: p_j
    \right\}
\end{equation*}
Scrivendo il costo della soluzione come $m(i,j) = c\left( T_{i..j}^* \right)$ si ottiene:
\begin{equation*}
    m(i,j)
    =
    \begin{cases}
    0 & i=j \\    
    \displaystyle\min_{1 \leq \bar{k} < j}
    \left\{ 
    m(i, \bar{k})
    +
    m(\bar{k}+1,j)
    + p_{i-1} \: p_{\bar{k}} \: p_j
    \right\}
    & i \neq j
    \end{cases}
\end{equation*}
mentre per l'informazione addizionale per ricostruire la soluzione è sufficiente memorizzare il punto dove si spezza l'albero:
\begin{equation*}
    S(i,j)
    =
    \bar{k} =
    \displaystyle\argmin_{1 \leq \bar{k} < j}
    \left\{ 
    m(i, \bar{k})
    +
    m(\bar{k}+1,j)
    + p_{i-1} \: p_{\bar{k}} \: p_j
    \right\}
\end{equation*}
La presenza di problemi ripetuti si verifica facilmente: l'istanza $(1,4)$ genera, nel momento dell'accumulazione del massimo:
per $k=1$ i problemi $(1,1)$ e $(2,4)$,
per $k=2$ i problemi $(1,2)$ e $(3,4)$,
per $k=3$ i problemi $(1,3)$ e $(4,4)$.
Il sottoproblema $(2,4)$ a sua volta genera le coppie di sottoproblemi 
$(2,2)$ e $(3,4)$,
$(2,3)$ e $(4,4)$,
per cui si nota già la ripetizione dell'istanza $(3,4)$.
L'approccio con la programmazione dinamica al problema può quindi migliorare l'efficienza rispetto al \emph{D\&C}.

\subsection{Implementazione dell'algoritmo \emph{MCM} iterativo}

L'ultimo problema da risolvere per poter implementare l'algoritmo in maniera \emph{bottom-up} è stabilire l'ordine di risoluzione delle sottoistanze.
Osservando la matrice dei costi $m[1..n, 1..n]$ \ref{eq:mcmmatcosti}, si nota che i casi base $i=j$ sono lungo la diagonale principale, e i casi non di base $i<j$ sono nella parte superiore della matrice.
Supponendo di star calcolando $m(i,j)$, i sottoproblemi che è necessario avere a disposizione sono: per la parte $m(i,k)$, quando $k=i$, il problema è sulla diagonale principale, e per valori crescenti di $k$, i problemi sono sulla riga $i$. Per l'addendo $m(k+1,j)$, quando $k=i$ è necessario il problema subito sotto, ossia $m(i+1,j)$, e per i successivi valori di $k$ devono essere a disposizione i valori sulla colonna $j$, fino ad arrivare alla diagonale principale. Le frecce indicano valori crescenti di $k$.
\begin{equation}
    \left[ 
        \begin{array}[H]{ccccccc}
            0 & & & & & & \\
            & 0 & & & & & \\
            & & i,i & i+1,i & \rightarrow & i,j & \\
            & & & 0 & & i+1,j & \\
            & & & & 0 & \downarrow & \\
            & & & & & j,j & \\
            & & & & & & 0\\
        \end{array}
    \right]
    \label{eq:mcmmatcosti}
\end{equation}
Procedendo per diagonali parallele alla principale, e riducendone la dimensione fino ad arrivare all'angolo, si possono calcolare gli elementi in un ordine coerente.
L'equazione di una generica diagonale è $l'=j'-i'+1$, che per $l'=1$ risulta la diagonale principale: $1=j'-i'+1 \rightarrow i'=j'$.
Il primo indice di iterazione sarà quindi $l$ crescente da $1$ a $n$. Fissato un generico $l$, gli indici della diagonale generica risultano
$(1,l)$, $(2,l+1)$ e così via, in generale: $(\bar{i}, \bar{i}+l-1)$. L'ultimo elemento viene raggiunto per $\bar{i}+l-1=n \rightarrow \bar{i}=n-l+1$.
Il valore di $\bar{j}$ discende da $l$ ed $\bar{i}$.

L'algoritmo per calcolare la parentesizzazione ottima di una catena di matrici risulta
\begin{algorithm}[H]
\caption{\emph{Matrix Chain Multiplication}}\label{alg:mcmit}
\begin{algorithmic}[1]
\Procedure{MCM}{$\vec{p}$}
    \State $n \gets \vec{p}.len$
    \For{$i \gets 1 $ to $ n $ }
        \State $m[i,i] \gets 0$
    \EndFor
    \For{$l \gets 2 $ to $ n $ }
        \For{$i \gets 2 $ to $ n-l+1 $ }
            \State $m[i,j] \gets +\infty$
            \label{alg:mcmit:initmij}
            \State $j \gets i+l-1$
            \For{$k \gets i $ to $ j-1 $ }
                \State $q \gets m[i,k]+m[k+1,j]+ p_{i-1} \, p_k \, p_j$
                \If{$q < m[i,j]$}
                    \State $m[i,j] \gets q$
                    \State $S[i,j] \gets k$
                \EndIf
            \EndFor
        \EndFor
    \EndFor
    \State return $m[1,n]$, $\vec{S}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Anche scorrendo la matrice secondo il \emph{reversed column major} gli elemento vengono visitati in ordine corretto:
\begin{algorithmic}[1]
    \For{$i \gets 1 $ to $ n $ }
        \For{$j \gets i $ down to $ 1 $ }
        \EndFor
    \EndFor
\end{algorithmic}

La complessità dell'algoritmo risulta:
\begin{align*}
    T_{MCM}(n) 
    &= \sum_{l=2}^{n} \sum_{i=1}^{n-l+1} \sum_{k=i}^{i+l-2} 2
    & i+l-2-i+1 = l-1 &\\
    &= \sum_{l=2}^{n} \sum_{i=1}^{n-l+1} 2 (l-1)
    & n-l+1-1+1 = n-l+1 &\\
    &= 2 \sum_{l=2}^{n} (l-1) (n-l+1)
    & l'=l-1 &\\
    &= 2 \sum_{l'=1}^{n-1} l' (n-l')
    \\
    &= 2 \left[ n \sum_{l'=1}^{n-1} l' - \sum_{l'=1}^{n-1} (l')^2\right]
    \labeq{A}
    \\
    &= 2 \left[ n \frac{n(n-1)}{2} - \frac{(n-1)n(2n-1)}{6}\right]
    \\
    &= n(n-1) \left[ n - \frac{2n-1}{3}\right]
    \\
    &= \frac{n(n-1)(n+1)}{3}
    \\
    &= \Theta \left( n^3 \right)
\end{align*}
Dove in $\labeq{A}$ è stata usata, istanziandola per $n-1$,
\begin{equation*}
    \sum_{i=1}^{n} i^2 = \frac{n(n+1)(2n+1)}{6}
\end{equation*}

Avendo a disposizione $S$, scrivere la parentesizzazione è semplice, osservando che
\begin{equation*}
    str\left( T_{i..j}^* \right) = < `(\textrm{'} , 
    str\left( T_{i..k}^* \right) ,
    str\left( T_{k+1..j}^* \right)
    , `)\textrm{'} > 
\end{equation*}
con $str\left( T_{i..i}^* \right) = `A_i\textrm{'} $ 
\begin{algorithm}[H]
\caption{\emph{Stampa della parentesizzazione}}\label{alg:mcmprint}
\begin{algorithmic}[1]
\Procedure{PRINT\_P}{$i,j,\vec{S}$}
    \If{$i = j$}
        \State \Call{print}{$`A_i\textrm{'}$}
        \State return
    \EndIf
    \State \Call{print}{`(\textrm{'}}
    \State \Call{PRINT\_P}{$i, S[i,j], \vec{S}$}
    \State \Call{PRINT\_P}{$S[i,j]+1, j, \vec{S}$}
    \State \Call{print}{`)\textrm{'}}
\EndProcedure
\end{algorithmic}
\end{algorithm}
Il costo della stampa si misura nel numero di caratteri che vengono stampati: essendo la parentesizzazione un albero pieno con $n$ foglie e $n-1$ nodi interni, contando due parentesi per nodo, verranno stampati $n+2(n-1) = \Theta(n)$ caratteri.

L'algoritmo \ref{alg:mcmit} viene utilizzato nel calcolo effettivo di una catena di moltiplicazioni tra matrici compatibili $\vec{A}=\left( A_1, \dots, A_n \right)$, descritta da $\vec{p}$, nel seguente modo, strutturato in maniera simile al PRINT\_P, utilizzando anche RECT\_MUL (\ref{alg:mulret}):
\begin{algorithm}[H]
\caption{\emph{Moltiplicazione ottima di matrici rettangolari}}\label{alg:mcmmul}
\begin{algorithmic}[1]
    \State $m, \vec{S} \gets \Call{MCM}{\vec{p}}$
    \Procedure{MCM\_MUL}{$\vec{A},\vec{S},i,j$}
    \If{$i = j$}
        \State return $A_i$
    \EndIf
    \State \Call{MCM\_MUL}{$\vec{A},\vec{S},i, S[i,j]$}
    \State \Call{MCM\_MUL}{$\vec{A},\vec{S},S[i,j]+1,j$}
    \State return \Call{RECT\_MUL}{$X,Y$}
\EndProcedure
\end{algorithmic}
\end{algorithm}
Il costo sarà proprio $m$, calcolato in MCM.
